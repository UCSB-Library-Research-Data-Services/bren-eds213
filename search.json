[
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Burnette. (2022). Managing environmental data: principles, techniques, and best practices. CRC Press. https://search.library.ucsb.edu/permalink/01UCSB_INST/1aqck9j/alma9917095897506531\nChapman, AD, & Grafton, O. (2008). Guide to Best Practices for Generalising Sensitive Species-Occurrence Data (v1.0). https://doi.org/10.15468/doc-b02j-gt10\nCrystal-Ornelas, R., Varadharajan, C., O’Ryan, D., Ramírez-Muñoz, J., Jones, M. B., Lehnert, K. A., … & Servilla, M. (2022). Enabling FAIR data in Earth and environmental science with community-centric (meta)data reporting formats. Scientific Data, 9(1), 700. https://doi.org/10.1038/s41597-022-01606-w\nJones, M. B., O’Brien, M., Mecum, B., Boettiger, C., Schildhauer, M., Maier, M., Whiteaker, T., Earl, S., & Chong, S. (2019). Ecological Metadata Language version 2.2.0. KNB Data Repository. https://doi.org/10.5063/F11834T2\nLabastida, I., & Margoni, T. (2020). Licensing FAIR Data for Reuse. Data Intelligence, 2(1-2), 199-207. https://doi.org/10.1162/dint_a_00042\nMcGovern, A., Ebert-Uphoff, I., Gagne, D., & Bostrom, A. (2022). Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science. Environmental Data Science, 1, E6. https://doi.org/10.1017/eds.2022.5\nRecknagel, F., & Michener, W. K. (Eds.). (2018). Ecological informatics: Data management and knowledge discovery (3rd ed.). Springer."
  },
  {
    "objectID": "resources.html#bibliography",
    "href": "resources.html#bibliography",
    "title": "Resources",
    "section": "",
    "text": "Burnette. (2022). Managing environmental data: principles, techniques, and best practices. CRC Press. https://search.library.ucsb.edu/permalink/01UCSB_INST/1aqck9j/alma9917095897506531\nChapman, AD, & Grafton, O. (2008). Guide to Best Practices for Generalising Sensitive Species-Occurrence Data (v1.0). https://doi.org/10.15468/doc-b02j-gt10\nCrystal-Ornelas, R., Varadharajan, C., O’Ryan, D., Ramírez-Muñoz, J., Jones, M. B., Lehnert, K. A., … & Servilla, M. (2022). Enabling FAIR data in Earth and environmental science with community-centric (meta)data reporting formats. Scientific Data, 9(1), 700. https://doi.org/10.1038/s41597-022-01606-w\nJones, M. B., O’Brien, M., Mecum, B., Boettiger, C., Schildhauer, M., Maier, M., Whiteaker, T., Earl, S., & Chong, S. (2019). Ecological Metadata Language version 2.2.0. KNB Data Repository. https://doi.org/10.5063/F11834T2\nLabastida, I., & Margoni, T. (2020). Licensing FAIR Data for Reuse. Data Intelligence, 2(1-2), 199-207. https://doi.org/10.1162/dint_a_00042\nMcGovern, A., Ebert-Uphoff, I., Gagne, D., & Bostrom, A. (2022). Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science. Environmental Data Science, 1, E6. https://doi.org/10.1017/eds.2022.5\nRecknagel, F., & Michener, W. K. (Eds.). (2018). Ecological informatics: Data management and knowledge discovery (3rd ed.). Springer."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data through:\n\nConsultations\nLong-term engagements\nInstructional workshops\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), and sustainably preservable, and that researchers receive scholarly credit for sharing and publishing data.\nContact us if you have any questions: rds@library.ucsb.edu"
  },
  {
    "objectID": "modules/week09/case-d.html",
    "href": "modules/week09/case-d.html",
    "title": "eds213",
    "section": "",
    "text": "Read the scenario and answer the questions based on the weekly readings and the lecture:\nTina, a researcher working on coastal vulnerability analysis in Southern California, acquired LiDAR data from a vendor in 2017. Based on the acquired dataset, she submitted a paper to a high-impact academic journal early this year. The paper was accepted but is pending publication until Tina complies with the mandate of sharing supporting data and associated documentation in an open repository. While inspecting the data documentation, Max, the repository data manager, noticed that the files included raw and processed data from a vendor; however, no explicit declaration of authorization to share the data was included in the submission package. Tina presented an invoice of $20,000 USD certifying that she obtained the data and said she was told verbally that the data was not subject to any use restrictions."
  },
  {
    "objectID": "modules/week09/case-d.html#instructions",
    "href": "modules/week09/case-d.html#instructions",
    "title": "eds213",
    "section": "",
    "text": "Read the scenario and answer the questions based on the weekly readings and the lecture:\nTina, a researcher working on coastal vulnerability analysis in Southern California, acquired LiDAR data from a vendor in 2017. Based on the acquired dataset, she submitted a paper to a high-impact academic journal early this year. The paper was accepted but is pending publication until Tina complies with the mandate of sharing supporting data and associated documentation in an open repository. While inspecting the data documentation, Max, the repository data manager, noticed that the files included raw and processed data from a vendor; however, no explicit declaration of authorization to share the data was included in the submission package. Tina presented an invoice of $20,000 USD certifying that she obtained the data and said she was told verbally that the data was not subject to any use restrictions."
  },
  {
    "objectID": "modules/week09/case-d.html#questions",
    "href": "modules/week09/case-d.html#questions",
    "title": "eds213",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nMax should advise Tina to acquire explicit permission from the data vendor to share the data.\n\nTrue\nFalse\n\n\n\nQuestion 2\nBecause Tina paid for the data, Max can move forward with the data publication without infringing on any legal and ethical aspects.\n\nTrue\nFalse\n\n\n\nQuestion 3\nIf Tina does not acquire explicit permission from the vendor to share the data, Max can’t publish the data in the repository.\n\nTrue\nFalse\n\n\n\nQuestion 4\nIf Tina does not acquire written permission to share the data, Max can suggest Tina share only aggregated data.\n\nTrue\nFalse"
  },
  {
    "objectID": "modules/week09/index-09.html",
    "href": "modules/week09/index-09.html",
    "title": "Week 9 - Ethical and responsible data management",
    "section": "",
    "text": "Understand fundamental ethical and responsible data management principles, focusing on the importance of data documentation, preventing bias and harm, properly handling sensitive data, ownership, and licensing issues\nApply ethical and responsible data management principles to real-world scenarios"
  },
  {
    "objectID": "modules/week09/index-09.html#learning-objectives",
    "href": "modules/week09/index-09.html#learning-objectives",
    "title": "Week 9 - Ethical and responsible data management",
    "section": "",
    "text": "Understand fundamental ethical and responsible data management principles, focusing on the importance of data documentation, preventing bias and harm, properly handling sensitive data, ownership, and licensing issues\nApply ethical and responsible data management principles to real-world scenarios"
  },
  {
    "objectID": "modules/week09/index-09.html#slides",
    "href": "modules/week09/index-09.html#slides",
    "title": "Week 9 - Ethical and responsible data management",
    "section": "Slides",
    "text": "Slides\nslides-09.pptx"
  },
  {
    "objectID": "modules/week09/index-09.html#preparatory-work-for-in-class-exercise",
    "href": "modules/week09/index-09.html#preparatory-work-for-in-class-exercise",
    "title": "Week 9 - Ethical and responsible data management",
    "section": "Preparatory work for in-class exercise",
    "text": "Preparatory work for in-class exercise\nRequired readings\n\nBoté, J. J., & Térmens, M. (2019). Reusing data: Technical and ethical challenges. DESIDOC Journal of Library & Information Technology, 39(6) http://hdl.handle.net/2445/151341\nMcGovern, A., Ebert-Uphoff, I., Gagne, D., & Bostrom, A. (2022). Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science. Environmental Data Science, 1, E6. https://doi.org/10.1017/eds.2022.5\n\nQuestions to consider: (no need to answer the questions on paper)\n\nBoté and Térmens (2019) present some technical and ethical challenges researchers may face when using pre-existing data. Based on the lecture and our group discussion on day 3, in what ways do you think the technical challenges highlighted in this paper can also have ethical implications?\nCan you think of ways some of the ethical concerns presented by Boté and Térmens (2019) can affect other stages of the data lifecycle other than reuse?\nBased on concrete examples, McGovern et al. (2022) discuss how AI and ML can go wrong for environmental sciences. While the community is still grappling with these challenges, what responsible and ethical data management practices could help to minimize such problems?\nReflecting on both settings of readings, can you identify any ethical concerns that resonate with your capstone project? How have you or do you plan to mitigate them?\n\nAdditional suggested readings are noted in the slides."
  },
  {
    "objectID": "modules/week09/index-09.html#in-class-exercise",
    "href": "modules/week09/index-09.html#in-class-exercise",
    "title": "Week 9 - Ethical and responsible data management",
    "section": "In-class exercise",
    "text": "In-class exercise\nInstructions\n\nFind a partner (preferably someone from a different capstone project team).\nRead the cases listed below, discuss them with your partner, and answer the quizzes individually.\n\nCase Study A: Containing the flames of bias in machine learning\nCase Study B: The caveat is the caviar: navigating ethics to protect endangered river wildlife\nCase Study C: To reuse or not reuse, that is the key question!\nCase Study D: Navigating the complexities of ownership zones\n\nSwitch partners and discuss your answers.\nReflect on your answers and change them or refine the open ones if you’d like.\nSubmit your final answers (individually) before the end of the class."
  },
  {
    "objectID": "modules/week09/case-a.html",
    "href": "modules/week09/case-a.html",
    "title": "Case Study A: Containing the flames of bias in machine learning",
    "section": "",
    "text": "Read the scenario and answer questions based on the weekly readings and the lecture:\nWildfires have become increasingly common and destructive in many regions worldwide, causing significant environmental and social problems. In response, many communities have implemented fire prevention and management strategies, including using machine learning (ML) algorithms to predict and mitigate the risk of wildfires.\nOakdale, located in a densely forested area in British Columbia, Canada, has implemented an ML algorithm to predict the risk of wildfires and prioritize fire prevention resources. The algorithm uses a variety of inputs, including historical fire data, weather patterns, topography, and vegetation coverage, to generate a risk score for each area of the city. However, after several months of using the algorithm, city officials noticed that specific neighborhoods with low-income and minority populations consistently receive lower risk scores than other areas with very similar environmental conditions. Upon closer examination of those patterns in the data, they realized that the historical data used to train the algorithm was heavily concentrated on more affluent and predominantly white neighborhoods, resulting in a skewed view of the fire risks for the whole city."
  },
  {
    "objectID": "modules/week09/case-a.html#instructions",
    "href": "modules/week09/case-a.html#instructions",
    "title": "Case Study A: Containing the flames of bias in machine learning",
    "section": "",
    "text": "Read the scenario and answer questions based on the weekly readings and the lecture:\nWildfires have become increasingly common and destructive in many regions worldwide, causing significant environmental and social problems. In response, many communities have implemented fire prevention and management strategies, including using machine learning (ML) algorithms to predict and mitigate the risk of wildfires.\nOakdale, located in a densely forested area in British Columbia, Canada, has implemented an ML algorithm to predict the risk of wildfires and prioritize fire prevention resources. The algorithm uses a variety of inputs, including historical fire data, weather patterns, topography, and vegetation coverage, to generate a risk score for each area of the city. However, after several months of using the algorithm, city officials noticed that specific neighborhoods with low-income and minority populations consistently receive lower risk scores than other areas with very similar environmental conditions. Upon closer examination of those patterns in the data, they realized that the historical data used to train the algorithm was heavily concentrated on more affluent and predominantly white neighborhoods, resulting in a skewed view of the fire risks for the whole city."
  },
  {
    "objectID": "modules/week09/case-a.html#questions",
    "href": "modules/week09/case-a.html#questions",
    "title": "Case Study A: Containing the flames of bias in machine learning",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nThis case presents an ethical concern primarily associated with what?\n\n\nQuestion 2\nAccording to McGovern et al. (2022), which AI/ML issues can be identified in this case study? Justify your answer.\n\n\nQuestion 3\nSuppose you were hired as a consultant by Oakdale’s city officials. Which of the following recommendations would you give them to prevent perpetuating bias and inequitable outcomes? (Select all that apply)\n\nImplement transparency measures that make the algorithms’ decision-making processes more visible and understandable to stakeholders. This may include clarifying how decisions are made, sharing data sources, and providing access to model outputs. Fully document any limitations and shortcomings of the model and data.\nInvolve diverse stakeholders in the algorithm development and testing, including individuals from communities whose outputs may disproportionately impact. This can help identify and address potential biases and ensure that the algorithm is designed with the interests of all community members in mind.\nContinue using the algorithm as the official decision-making source until the re-training is completed. After all, ML methods are more efficient than traditional fire prevention strategies (e.g., fire breaks and vegetation management)."
  },
  {
    "objectID": "modules/week01/index-01.html",
    "href": "modules/week01/index-01.html",
    "title": "Week 1 - Relational databases and data modeling",
    "section": "",
    "text": "Benefits of relational databases\nRelational data model and SQL data definition\nData modeling"
  },
  {
    "objectID": "modules/week01/index-01.html#learning-objectives",
    "href": "modules/week01/index-01.html#learning-objectives",
    "title": "Week 1 - Relational databases and data modeling",
    "section": "",
    "text": "Benefits of relational databases\nRelational data model and SQL data definition\nData modeling"
  },
  {
    "objectID": "modules/week01/index-01.html#slides",
    "href": "modules/week01/index-01.html#slides",
    "title": "Week 1 - Relational databases and data modeling",
    "section": "Slides",
    "text": "Slides\nslides-01.pptx"
  },
  {
    "objectID": "modules/week01/index-01.html#resources",
    "href": "modules/week01/index-01.html#resources",
    "title": "Week 1 - Relational databases and data modeling",
    "section": "Resources",
    "text": "Resources\n\nhttps://learning.nceas.ucsb.edu/2023-06-delta/session_09.html\n\nVery brief introduction to data modeling, ties into “tidy data.”\n\nChristoph Wohner, Johannes Peterseil, and Hermann Klug (2022). Designing and implementing a data model for describing environmental monitoring and research sites. Ecological Informatics 70, 101708.\nhttps://doi.org/10.1016/j.ecoinf.2022.101708\n\nGood case study.\n\nGerald A. Burnette (2022). Managing environmental data: principles, techniques, and best practices. CRC Press.\nAccess via Library Catalog\n\nComprehensive text, specific to environmental sciences.\n\nGraeme C. Simsion and Graham C. Witt (2005). Data Modeling Essentials. 3rd ed. Amsterdam: Morgan Kaufmann.\nAccess via Library Catalog\nGoogle Books\n\nComprehensive text, not specific to the environmental sciences.\n\nHartmut Hebbel (1994). Environmental data modeling. Annals of Operations Research 54, 263-278.\nhttps://doi.org/10.1007/BF02031737\n\nA broader view of data organization.\n\nJeffrey D. Ullman and Jennifer Widom (2008). A First Course in Database Systems. 3rd ed. Upper Saddle River, NJ: Pearson/Prentice Hall.\nAccess via Library Catalog\n\nComplete but theoretical introduction to relational databases, data modeling, and relational algebra."
  },
  {
    "objectID": "modules/week01/index-01.html#homework",
    "href": "modules/week01/index-01.html#homework",
    "title": "Week 1 - Relational databases and data modeling",
    "section": "Homework",
    "text": "Homework\nCreate an ER diagram\nData modeling exercise"
  },
  {
    "objectID": "modules/week05/index-05.html",
    "href": "modules/week05/index-05.html",
    "title": "Week 5 - Bash Programming & Advanced database topics",
    "section": "",
    "text": "Introduction to bash to write script\nExposure to the concepts of:\n\nConcurrency and transactions\nBackups\nIndexes"
  },
  {
    "objectID": "modules/week05/index-05.html#learning-objectives",
    "href": "modules/week05/index-05.html#learning-objectives",
    "title": "Week 5 - Bash Programming & Advanced database topics",
    "section": "",
    "text": "Introduction to bash to write script\nExposure to the concepts of:\n\nConcurrency and transactions\nBackups\nIndexes"
  },
  {
    "objectID": "modules/week05/index-05.html#bash-programming",
    "href": "modules/week05/index-05.html#bash-programming",
    "title": "Week 5 - Bash Programming & Advanced database topics",
    "section": "Bash programming",
    "text": "Bash programming\nTen Bash Essentials"
  },
  {
    "objectID": "modules/week05/index-05.html#bash-resources",
    "href": "modules/week05/index-05.html#bash-resources",
    "title": "Week 5 - Bash Programming & Advanced database topics",
    "section": "Bash resources",
    "text": "Bash resources\n\nhttps://swcarpentry.github.io/shell-novice/\n\nGood Carpentry lesson, our lesson on Bash is drawn from this.\n\nhttps://www.gnu.org/software/bash/manual/\n\nThe official user manual.\n\nhttps://www.pcwdld.com/bash-cheat-sheet\n\nA cheat sheet that is better than most.\n\nhttps://www.shellcheck.net/\n\nIndispensable tool for writing scripts.\n\nhttp://mywiki.wooledge.org/BashPitfalls\n\nHow to do things the right way in Bash."
  },
  {
    "objectID": "modules/week05/index-05.html#advanced-topics",
    "href": "modules/week05/index-05.html#advanced-topics",
    "title": "Week 5 - Bash Programming & Advanced database topics",
    "section": "Advanced topics",
    "text": "Advanced topics"
  },
  {
    "objectID": "modules/week05/index-05.html#slides",
    "href": "modules/week05/index-05.html#slides",
    "title": "Week 5 - Bash Programming & Advanced database topics",
    "section": "Slides",
    "text": "Slides\nslides-05.pptx"
  },
  {
    "objectID": "modules/week05/index-05.html#instructor-materials",
    "href": "modules/week05/index-05.html#instructor-materials",
    "title": "Week 5 - Bash Programming & Advanced database topics",
    "section": "Instructor materials",
    "text": "Instructor materials\nInstructor notes\nInstructor files:\n\nconcurrency.zip\nindexes.zip"
  },
  {
    "objectID": "modules/week05/index-05.html#resources",
    "href": "modules/week05/index-05.html#resources",
    "title": "Week 5 - Bash Programming & Advanced database topics",
    "section": "Resources",
    "text": "Resources\n\nJeffrey D. Ullman and Jennifer Widom (2008). A First Course in Database Systems. 3rd ed. Upper Saddle River, NJ: Pearson/Prentice Hall.\nAccess via Library Catalog\n\nComplete but theoretical introduction to relational databases, data modeling, and relational algebra."
  },
  {
    "objectID": "modules/week05/index-05.html#homework",
    "href": "modules/week05/index-05.html#homework",
    "title": "Week 5 - Bash Programming & Advanced database topics",
    "section": "Homework",
    "text": "Homework\nBash scripting\nWhat makes a good index?"
  },
  {
    "objectID": "modules/week05/instructor-notes-05.html",
    "href": "modules/week05/instructor-notes-05.html",
    "title": "Week 5 - Instructor notes",
    "section": "",
    "text": "Files\n\n\n\n\n\n\nconcurrency.db\n\n\n\n\nconcurrency.sql\n\n\n\n\nexperiment.sh\n\n\n\n\nworker.py\n\n\n\n\n\nNo matching items\n\n\nThe above files, which can be downloaded in toto as concurrency.zip, can be used to demonstrate the necessity of using transactions and locking to avoid race conditions. concurrency.db is an SQLite database created by running the commands in concurrency.sql. It contains a single value \\(V\\) in a single row in a single table.\nworker.py is a Python program that adds a specified delta value to \\(V\\) a specified number of times. For example, python worker.py 500 1 adds 1 to \\(V\\) 500 times. By default it does no locking at all, but if a wt (“with transactions”) argument is added, as in python worker.py 500 1 wt, it wraps each update in a transaction.\nexperiment.sh is a Bash script that resets \\(V\\) to 0, runs two instances of worker.py concurrently, one adding +1 to \\(V\\) 500 times and the other adding -1 to \\(V\\) 500 times, and then prints the final value of \\(V\\). By default it does no locking, but as with worker.py, if it is run with a wt argument then transactions and locking are employed. With no race conditions the final value of \\(V\\) will of course be 0, but if race conditions occur, \\(V\\) will likely have a random nonzero value.\nThe database configuration, the connection details, and the timing delays have all been designed to make race conditions very likely to occur. (It is suprisingly difficult to get race conditions to occur due to SQLite’s default locking and due to artifacts of operating system timeslicing.) This experimental setup has been tested on MacOS, Windows, and several Unix platforms and consistently yields the desired result. Here’s an example run without transactions:\n% ./experiment.sh\nvalue = 14\n% ./experiment.sh\nvalue = 15\n% ./experiment.sh\nvalue = -2\nAnd adding transactions and locking:\n% ./experiment.sh wt\nvalue = 0\n% ./experiment.sh wt\nvalue = 0\n% ./experiment.sh wt\nvalue = 0\nIt should take less than 60 seconds to run an experiment."
  },
  {
    "objectID": "modules/week05/instructor-notes-05.html#concurrency-demonstration",
    "href": "modules/week05/instructor-notes-05.html#concurrency-demonstration",
    "title": "Week 5 - Instructor notes",
    "section": "",
    "text": "Files\n\n\n\n\n\n\nconcurrency.db\n\n\n\n\nconcurrency.sql\n\n\n\n\nexperiment.sh\n\n\n\n\nworker.py\n\n\n\n\n\nNo matching items\n\n\nThe above files, which can be downloaded in toto as concurrency.zip, can be used to demonstrate the necessity of using transactions and locking to avoid race conditions. concurrency.db is an SQLite database created by running the commands in concurrency.sql. It contains a single value \\(V\\) in a single row in a single table.\nworker.py is a Python program that adds a specified delta value to \\(V\\) a specified number of times. For example, python worker.py 500 1 adds 1 to \\(V\\) 500 times. By default it does no locking at all, but if a wt (“with transactions”) argument is added, as in python worker.py 500 1 wt, it wraps each update in a transaction.\nexperiment.sh is a Bash script that resets \\(V\\) to 0, runs two instances of worker.py concurrently, one adding +1 to \\(V\\) 500 times and the other adding -1 to \\(V\\) 500 times, and then prints the final value of \\(V\\). By default it does no locking, but as with worker.py, if it is run with a wt argument then transactions and locking are employed. With no race conditions the final value of \\(V\\) will of course be 0, but if race conditions occur, \\(V\\) will likely have a random nonzero value.\nThe database configuration, the connection details, and the timing delays have all been designed to make race conditions very likely to occur. (It is suprisingly difficult to get race conditions to occur due to SQLite’s default locking and due to artifacts of operating system timeslicing.) This experimental setup has been tested on MacOS, Windows, and several Unix platforms and consistently yields the desired result. Here’s an example run without transactions:\n% ./experiment.sh\nvalue = 14\n% ./experiment.sh\nvalue = 15\n% ./experiment.sh\nvalue = -2\nAnd adding transactions and locking:\n% ./experiment.sh wt\nvalue = 0\n% ./experiment.sh wt\nvalue = 0\n% ./experiment.sh wt\nvalue = 0\nIt should take less than 60 seconds to run an experiment."
  },
  {
    "objectID": "modules/week05/instructor-notes-05.html#index-demonstration-and-homework",
    "href": "modules/week05/instructor-notes-05.html#index-demonstration-and-homework",
    "title": "Week 5 - Instructor notes",
    "section": "Index demonstration and homework",
    "text": "Index demonstration and homework\n\n\n\n\n\nFiles\n\n\n\n\n\n\nhw-05-1.qmd\n\n\n\n\nbuild-big-db.sql\n\n\n\n\ngenerate-rows\n\n\n\n\nindex-timer\n\n\n\n\nquery.sql\n\n\n\n\nsample-output.csv\n\n\n\n\n\nNo matching items\n\n\nThe above files, which can be downloaded in toto as indexes.zip, create a large SQLite database to be used for the homework assignment. The database is essentially the same as has been used throughout the course to date, but contains a Bird_nests table having 1 million rows. The rows are valid (i.e., satisfy all table constraints) but have randomly generated values. The rows are also padded so that the database size is inflated to 4GB.\nquery.sql is a query that returns exactly one row from this enlarged Bird_nests table. The design is such that placing indexes on different column(s) of the table will result in markedly different performance improvements. Furthermore, query performance can be seen to have a clear relationship to index cardinality.\nTo build the database, create a copy of the database from week 7 and then run build-big-db.sql on it as shown below. The SQL script internally invokes the generate-rows Python script.\ncp .../week7/database.db db/database-long-wide.db\nsqlite3 -init build-big-db.sql db/database-long-wide.db\nindex-timer is a Bash script that is essentially what the homework problem asks the students to use, and what the students should have developed in assignment Week 5 - Create a test harness. Running experiments as described in the homework problem should produce data resembling the following:\n\n\n\n\n\nindex\naverage_time\nnum_distinct_values\n\n\n\n\nnone\n0.700\n1\n\n\nSite\n0.090\n16\n\n\nSpecies\n0.023\n99\n\n\nYear\n0.026\n66\n\n\nObserver\n0.023\n269\n\n\nageMethod\n0.400\n3\n\n\nSite,Species\n0.022\n1584\n\n\nSite,Year\n0.022\n1056\n\n\nSite,Observer\n0.022\n4304\n\n\nSite,ageMethod\n0.040\n48\n\n\nSpecies,Year\n0.023\n6534\n\n\nSpecies,Observer\n0.022\n26631\n\n\nSpecies,ageMethod\n0.022\n297\n\n\nYear,Observer\n0.022\n17754\n\n\nYear,ageMethod\n0.022\n198\n\n\nObserver,ageMethod\n0.023\n807\n\n\n\n\n\nWhich, when plotted, reveals a clear relationship:"
  },
  {
    "objectID": "modules/week05/hw-04-3.html",
    "href": "modules/week05/hw-04-3.html",
    "title": "Week 4 - Create a test harness",
    "section": "",
    "text": "Part 1\nA common task is to create a script — a “test harness” — that will allow you to test something (some code, an algorithm, a model, etc.) by running it repeatedly, perhaps under varying conditions. Example applications include machine learning using different hyperparameters and Monte Carlo simulation using different random seeds. In this assignment you will create a Bash script that will allow you to time how long it takes to run an SQL query. You will be using this script in the last week of class to explore the performance effects of indexes, so hang on to it.\nYour script will be invoked like so:\n% bash my_sqlite_tester.sh label num_reps query db_file csv_file\n\n  Arguments:\n      label:    explanatory label that will be output\n      num_reps: number of repetitions\n      query:    SQL query to run\n      db_file:  database file\n      csv_file: CSV file to create or append to\nFor example, you might run:\n% bash my_sqlite_tester.sh with_index_a 1000 'SELECT COUNT(*) FROM Bird_nests' database.db timings.csv\nIn this example your script would run the given query (i.e., SELECT COUNT(*) FROM Bird_nests) on the given database (database.db) 1000 times. If the total time that took was 3 seconds, your script will divide that time by the number of repetitions (1000) and compute that each SQLite invocation took 0.003 seconds. Finally, your script will append the following record to the CSV tile timings.csv:\nwith_index_a,0.003\nConceptually, your script will look something like this (this is pseudocode):\nget current time and store it\nloop num_reps times\n    sqlite3 db_file query\nend loop\nget current time\ncompute elapsed time\ndivide elapsed time by num_reps\nwrite output\nI would like you to follow the above pseudocode because this exercise is fundamentally about using certain Bash features.\n(BTW, is this a fair way to time queries? Yes and no. Clearly it’s a bit unfair that we are counting the overhead of repeatedly firing up SQLite and opening the database file. And there may be other processing going on on the machine that affects the wall-clock time that we’re measuring here. On the other hand, databases are almost always I/O-bound and not compute-bound, that is, their performance is primarily limited by the time it takes to read data into memory from disk, and because of that, wall-clock time can be a better measure than CPU time.)\nWe’re doing this in Bash, so every one of these steps is a challenge (it is okay to hate Bash). So here are a lot of tips and hints:\n\nGetting the current time: try date +%s or use the magic SECONDS variable (do a man bash to read about it).\nLooping: see below.\nIn executing sqlite3, be sure to appropriately quote the query.\nComputing elapsed time: use Bash arithmetic.\nDivision. Bash does not support floating point numbers, so you’ll have to use a helper program. Take yer pick depending on what’s available to you. Let’s say you want to divide 10 by 3 (you will want to reference variables in your computation, but here I’m just illustrating dividing two literal numbers). You might say something like:\n\nelapsed=$(echo \"scale=7; 10/3\" | bc)\nelapsed=$(echo \"10/3\" | awk -F / '{print $1/$2}')\nelapsed=$(python -c \"print(10/3)\")\nelapsed=$(perl -l -e \"print 10/3\")\n\nOutput: be sure to use the appropriate I/O redirection.\n\nTwo approaches to looping. If you want to do something 10 times, you can use a while loop that increments a counter:\ni=0\nwhile [ $i -lt 10 ]; do\n    echo \"this is loop iteration $i\"\n    i=$((i+1))\ndone\nOr, you can use seq. Generally you can use a for loop to loop through a list of items like so:\nfor i in 0 1 2 3 4 5 6 7 8 9; do\n    echo \"this is loop iteration $i\"\ndone\nBut you can use seq to generate a list of numbers of a desired length:\nfor i in $(seq 10); do\n    echo \"this is loop iteration $i\"\ndone\nAnd some more advice. The reason for making the number of repetitions an argument to this script, as opposed to a fixed constant, is that you may have to adjust the number of repetitions depending how fast the query is. The date command and SECONDS variable have a resolution of only 1 second, so if running the query 10 times still fits in under 1 second your elapsed time will show up as 0. You may need to run a query 100 or 1000 times or more to get positive elapsed times, and to get more precision.\nAlso, we don’t actually care about the output from the query here. I would keep the output while you’re debugging your script (so that you can verify that SQLite is being run repeatedly), but once you feel your script is working you can redirect SQLite’s output and error streams to /dev/null.\nFinally, it is highly recommended that you upload your script to https://www.shellcheck.net. You’re not required to follow its advice, but I have found its advice to be enlightening.\nPlease submit your Bash script.\n\n\nPart 2\nIn class we looked at three ways to find out which species we do not have nest data for. Method using NOT IN:\nSELECT Code\n    FROM Species\n    WHERE Code NOT IN (SELECT DISTINCT Species FROM Bird_nests);\nMethod using an outer join:\nSELECT Species.Code\n    FROM Bird_nests RIGHT JOIN Species\n    ON Bird_nests.Species = Species.Code\n    WHERE Bird_nests.Nest_ID IS NULL;\nMethod using a set operation:\nSELECT Code FROM Species\nEXCEPT\nSELECT DISTINCT Species FROM Bird_nests;\nUse your test harness to time these three queries. Report back how many repetitions you had to use to get good timings, the query times, and which method is fastest."
  },
  {
    "objectID": "modules/week05/hw-04-2.html",
    "href": "modules/week05/hw-04-2.html",
    "title": "Week 4 - Create a trigger",
    "section": "",
    "text": "For this assignment, you must use week5/database.db in the class data GitHub repository!\nThe Bird_eggs table uniquely identifies each egg by a pair (Nest_ID, Egg_num). The egg numbers for a given nest always have the sequential values 1, 2, 3, 4, etc. For example, there are 3 eggs in nest 14eabaage01:\nSELECT * FROM Bird_eggs WHERE Nest_ID = '14eabaage01';\n┌───────────┬──────┬──────┬─────────────┬─────────┬────────┬───────┐\n│ Book_page │ Year │ Site │   Nest_ID   │ Egg_num │ Length │ Width │\n├───────────┼──────┼──────┼─────────────┼─────────┼────────┼───────┤\n│ b14.6     │ 2014 │ eaba │ 14eabaage01 │ 1       │ 39.14  │ 33.0  │\n│ b14.6     │ 2014 │ eaba │ 14eabaage01 │ 2       │ 41.51  │ 33.39 │\n│ b14.6     │ 2014 │ eaba │ 14eabaage01 │ 3       │ 48.29  │ 33.4  │\n└───────────┴──────┴──────┴─────────────┴─────────┴────────┴───────┘\n\nPart 1\nWhen inserting a new egg measurement, wouldn’t it be nice if the database just automatically filled in the next sequential egg number for us? Indeed, we can accomplish this with an AFTER INSERT trigger that does an UPDATE.\nThe schema for the database has been modified for this exercise so that Egg_num can be NULL and furthermore has a default value of NULL. This means we can insert a row without supplying an egg number. For example, we might say:\nINSERT INTO Bird_eggs\n    (Book_page, Year, Site, Nest_ID, Length, Width)\n    VALUES ('b14.6', 2014, 'eaba', '14eabaage01', 12.34, 56.78);\nIf this were the first egg measurement for this particular nest, immediately after the insert we would see:\n.nullvalue -NULL-\nSELECT * FROM Bird_eggs WHERE Nest_ID = '14eabaage01';\n┌───────────┬──────┬──────┬─────────────┬─────────┬────────┬───────┐\n│ Book_page │ Year │ Site │   Nest_ID   │ Egg_num │ Length │ Width │\n├───────────┼──────┼──────┼─────────────┼─────────┼────────┼───────┤\n│ b14.6     │ 2014 │ eaba │ 14eabaage01 │ -NULL-  │ 12.34  │ 56.78 │\n└───────────┴──────┴──────┴─────────────┴─────────┴────────┴───────┘\nIf there were already some egg measurements for this nest (as in fact there are), immediately after the insert we would see:\n.nullvalue -NULL-\nSELECT * FROM Bird_eggs WHERE Nest_ID = '14eabaage01';\n┌───────────┬──────┬──────┬─────────────┬─────────┬────────┬───────┐\n│ Book_page │ Year │ Site │   Nest_ID   │ Egg_num │ Length │ Width │\n├───────────┼──────┼──────┼─────────────┼─────────┼────────┼───────┤\n│ b14.6     │ 2014 │ eaba │ 14eabaage01 │ 1       │ 39.14  │ 33.0  │\n│ b14.6     │ 2014 │ eaba │ 14eabaage01 │ 2       │ 41.51  │ 33.39 │\n│ b14.6     │ 2014 │ eaba │ 14eabaage01 │ 3       │ 48.29  │ 33.4  │\n│ b14.6     │ 2014 │ eaba │ 14eabaage01 │ -NULL-  │ 12.34  │ 56.78 │\n└───────────┴──────┴──────┴─────────────┴─────────┴────────┴───────┘\nYour job is to create a trigger that will fire an UPDATE statement that will fill in a value for Egg_num in either situation above.\nYour trigger will have the form\nCREATE TRIGGER egg_filler\n    AFTER INSERT ON Bird_eggs\n    FOR EACH ROW\n    BEGIN\n        UPDATE ...;\n    END;\nA word of warning. Notice the two semicolons above: the UPDATE statement must be terminated by a semicolon, and the CREATE TRIGGER statement must be terminated by a semicolon.\nThe crux is in the UPDATE statement. Recall from class that in your UPDATE statement you can refer to the values just inserted as new.Book_page, new.Year, new.Site, new.Nest_ID, etc. For example, using the INSERT above, new.Nest_ID will have the value ‘14eabaage01’ and new.Length will have the value 12.34. You need to figure out:\n\nWhat column(s) to update. Well that’s easy, it’s just Egg_num.\nWhat Egg_num’s new value should be. Hint: the value can be computed from a SELECT statement. What SELECT statement could you use that will return the right value to use as the next sequential egg number?\nWhat row(s) to modify. Well, you want to modify just one row, the row that was just inserted. What WHERE clause could you use to identify this brand-new row? It has a unique signature.\n\nYou can try out your trigger by creating it, doing an INSERT, and then seeing what the rows for that particular nest look like. If your trigger doesn’t work for some reason, you may need to DROP TRIGGER egg_filler; before creating it again. As before, you will probably find it convenient to write your trigger code in a separate file, and load it into SQLite using the .read built-in command.\nPlease submit your SQL.\n\n\nPart 2\nWhy stop there? Recall that Book_page, Year, and Site all duplicate the information from the Bird_nests table. Wouldn’t it be nice if the database automatically filled in those values as well? Then we could just say:\nINSERT INTO Bird_eggs\n    (Nest_ID, Length, Width)\n    VALUES ('14eabaage01', 12.34, 56.78);\nThis can be accomplished by augmenting your previous trigger. Two options. One, you can add more UPDATE statements:\nCREATE TRIGGER egg_filler\n    AFTER INSERT ON Bird_eggs\n    FOR EACH ROW\n    BEGIN\n        UPDATE Bird_eggs SET Egg_num = (SELECT...) WHERE...;\n        UPDATE Bird_eggs SET Book_page = (SELECT...) WHERE...;\n        UPDATE Bird_eggs SET Year = (SELECT...) WHERE...;\n        etc.\n    END;\nOr two, you can add more clauses to a single UPDATE statement:\nCREATE TRIGGER egg_filler\n    AFTER INSERT ON Bird_eggs\n    FOR EACH ROW\n    BEGIN\n        UPDATE Bird_eggs\n            SET\n                Egg_num = (SELECT...),\n                Book_page = (SELECT...),\n                Year = (SELECT...),\n                etc.\n            WHERE ...;\n    END;\n(Honestly, this is not the most compact or efficient SQL, but sometimes it’s better sticking with a simple and understandable approach.)\nYou need to figure out what SELECT statements to use to find the values to insert. That is, given that you can reference new.Nest_ID, new.Length, and new.Width, what SELECT statements could you use to find the correct values for Book_page, Year, and Nest_ID?\nTry out your trigger, marvel at what you have automated, and submit your SQL.\n\n\nPart 3\nOkay, there isn’t a part 3. But wouldn’t it be nice to be able to insert egg measurements even more compactly? That is, instead of having to say:\nINSERT INTO Bird_eggs\n    (Nest_ID, Length, Width)\n    VALUES ('14eabaage01', 12.34, 56.78);\nwhat if you could just say:\nINSERT INTO Bird_eggs\n    VALUES ('14eabaage01', 12.34, 56.78);\nWell you can do that! It involves creating a view and adding an INSTEAD OF INSERT trigger on the view. If there’s time I’ll show an example in class."
  },
  {
    "objectID": "modules/week08/index-08.html",
    "href": "modules/week08/index-08.html",
    "title": "Week 8 - Sensitive data",
    "section": "",
    "text": "Understand the importance of protecting sensitive data and ensuring privacy and confidentiality.\nIdentify and evaluate established approaches and techniques for de-identifying and anonymizing data to mitigate the risk of re-identification.\nApply the acquired techniques while quantifying the information loss and utility, utilizing an R package."
  },
  {
    "objectID": "modules/week08/index-08.html#learning-goals",
    "href": "modules/week08/index-08.html#learning-goals",
    "title": "Week 8 - Sensitive data",
    "section": "",
    "text": "Understand the importance of protecting sensitive data and ensuring privacy and confidentiality.\nIdentify and evaluate established approaches and techniques for de-identifying and anonymizing data to mitigate the risk of re-identification.\nApply the acquired techniques while quantifying the information loss and utility, utilizing an R package."
  },
  {
    "objectID": "modules/week08/index-08.html#student-notes",
    "href": "modules/week08/index-08.html#student-notes",
    "title": "Week 8 - Sensitive data",
    "section": "Student notes",
    "text": "Student notes\nBefore class, install the sdcMicro package if you choose not to use the servers.\nIf testing out using sensitive data, make sure to launch it from RStudio, not from the online website."
  },
  {
    "objectID": "modules/week08/index-08.html#slides-and-other-materials",
    "href": "modules/week08/index-08.html#slides-and-other-materials",
    "title": "Week 8 - Sensitive data",
    "section": "Slides and other materials",
    "text": "Slides and other materials\nslides-08.pptx\nClass data GitHub repository, week 8 for in-class demo and homework"
  },
  {
    "objectID": "modules/week08/index-08.html#resources",
    "href": "modules/week08/index-08.html#resources",
    "title": "Week 8 - Sensitive data",
    "section": "Resources",
    "text": "Resources\n\nsdcMicro Documentation: https://sdcpractice.readthedocs.io/en/latest/intro.html\nsdcMicro Shiny app: https://sdcappdocs.readthedocs.io/en/latest/introsdcApp.html\n\nOther useful links can be found on slides."
  },
  {
    "objectID": "modules/week08/index-08.html#suggested-readings",
    "href": "modules/week08/index-08.html#suggested-readings",
    "title": "Week 8 - Sensitive data",
    "section": "Suggested readings",
    "text": "Suggested readings\n\nBledsoe, E. K., Burant, J. B., Higino, G. T., Roche, D. G., Binning, S. A., Finlay, K., … & Srivastava, D. S. (2022). Data rescue: saving environmental data from extinction. Proceedings of the Royal Society B, 289(1979), https://doi.org/10.1098/rspb.2022.0938\nBourgault, B., Tremblay, H.; Schloss, I.R.; Plante, S. & Archambault, P. (2017). “Commercially Sensitive” Environmental Data: A Case Study of Oil Seep Claims for the Old Harry Prospect in the Gulf of St. Lawrence, Canada. Case Studies in the Environment. https://doi.org/10.1525/cse.2017.sc.454841\nGehrke, J., Kifer, D., Machanavajjhala, A. (2011). ℓ-Diversity. In: van Tilborg, H.C.A., Jajodia, S. (eds) Encyclopedia of Cryptography and Security. Springer, Boston, MA. https://doi.org/10.1007/978-1-4419-5906-5_899\nSamarati, P., & Sweeney, L. (1998). Protecting privacy when disclosing information: k-anonymity and its enforcement through generalization and suppression. https://dataprivacylab.org/dataprivacy/projects/kanonymity/paper3.pdf"
  },
  {
    "objectID": "modules/week08/index-08.html#homework",
    "href": "modules/week08/index-08.html#homework",
    "title": "Week 8 - Sensitive data",
    "section": "Homework",
    "text": "Homework\nsdcMicro exercise"
  },
  {
    "objectID": "modules/week06/hw-06-1.html",
    "href": "modules/week06/hw-06-1.html",
    "title": "Week 6 - Little Bobby Tables",
    "section": "",
    "text": "View this classic XKCD cartoon:\n\n\n\nhttps://xkcd.com/327/\n\n\nFor the purposes of this problem you may assume that at some point the school’s software performs the query\nSELECT *\n    FROM Students\n    WHERE (name = '%s' AND year = 2023);\nwhere the student’s name is directly substituted for the %s. Explain exactly how Little Bobby Tables’ name can cause a catastrophe. Explain what happens to the AND year = 2023 clause in the query."
  },
  {
    "objectID": "modules/week06/hw-06-4.html",
    "href": "modules/week06/hw-06-4.html",
    "title": "Week 6 - Who were the winners?",
    "section": "",
    "text": "At the conclusion of the ASDN project the PIs decided to hand out first, second, and third prizes to the observers who measured the most eggs. Who won? Please use R and dbplyr to answer this question, and please submit your R code. Your code should print out:\n# Ordered by: desc(total_eggs)\n  Name            total_eggs\n  &lt;chr&gt;                &lt;int&gt;\n1 Vanessa Loverti        163\n2 Dylan Kessler           87\n3 Richard Lanctot         50\nYou’ll want to load database tables using statements such as:\negg_table &lt;- tbl(conn, \"Bird_eggs\")\nand then use tidyverse grouping, summarization, joining, and other functions to compute the desired result.\nAlso, take your final expression and pipe it into show_query(). If you used multiple R statements, did dbplyr create a temporary table, or did it manage to do everything in one query? Did it limit to the first three rows using an R expression or an SQL LIMIT clause?"
  },
  {
    "objectID": "modules/week07/hw-07.html",
    "href": "modules/week07/hw-07.html",
    "title": "Week 7 - EML record",
    "section": "",
    "text": "Each Capstone group should create an EML metadata record using the EML R Package for one of the analysis-ready datasets the project has generated or reused (consider a tabular dataset with 5-8 columns).\nAll groups must include the minimum elements covered in class, plus any additional attributes and descriptions required to describe your chosen dataset fully. For example, if attributes have specific units of measure, you should incorporate them into the code.\nExplore the package documentation, the README.md file you created for Week 4, and the important resources listed on Week 6 - Metadata standards.\nSubmit two files, the R script and the XML."
  },
  {
    "objectID": "modules/week03/hw-03-1.html",
    "href": "modules/week03/hw-03-1.html",
    "title": "Week 3 - SQL problem 1",
    "section": "",
    "text": "It’s a useful skill in life (I’m not being rhetorical, I really mean that, it’s a useful skill) to be able to construct an experiment to answer a hypothesis. Suppose you’re not sure what the AVG function returns if there are NULL values in the column being averaged. Suppose you either didn’t have access to any documentation, or didn’t trust it. What experiment could you run to find out what happens?\nThere are two parts to this problem.\n\nPart 1\nConstruct an SQL experiment to determine the answer to the question above. Does SQL abort with some kind of error? Does it ignore NULL values? Do the NULL values somehow factor into the calculation, and if so, how?\nI would suggest you start by creating a table (in the bird database, in a new database, in a transient in-memory database, doesn’t matter) with a single column that has data type REAL (for part 2 below, it must be REAL). You can make your table a temp table or not, your choice.\nCREATE TEMP TABLE mytable... ;\nNow insert some real numbers and at least one NULL into your table.\nINSERT INTO mytable... ;\n(Hmm, can you insert multiple rows at once, or do you have to do a separate INSERT for each row?)\nOnce you have your little table constructed, try doing an AVG on the column and see what is returned. What would the average be if the function ignored NULLs? What would the average be if it somehow factored them in? What is actually returned?\nPlease submit both your SQL and your answer to the question about how AVG operates in the presence of NULL values.\n\n\nPart 2\nIf SQL didn’t have an AVG function, you could compute the average value of a column by doing something like this on your table:\nSELECT SUM(mycolumn)/COUNT(*) FROM mytable;\nSELECT SUM(mycolumn)/COUNT(mycolumn) FROM mytable;\nWhich query above is correct? Please explain why.\nNow that you’re done with your table, you can delete it if desired:\nDROP TABLE mytable;"
  },
  {
    "objectID": "modules/week03/index-03.html",
    "href": "modules/week03/index-03.html",
    "title": "Week 3 - Structured Query Language (SQL) & DuckDB",
    "section": "",
    "text": "Understand the relationship of SQL to relational databases\nUnderstand how local databases differ from client/server databases\nUnderstand basic SQL syntax and statements\nBe able to answer basic questions about data"
  },
  {
    "objectID": "modules/week03/index-03.html#learning-objectives",
    "href": "modules/week03/index-03.html#learning-objectives",
    "title": "Week 3 - Structured Query Language (SQL) & DuckDB",
    "section": "",
    "text": "Understand the relationship of SQL to relational databases\nUnderstand how local databases differ from client/server databases\nUnderstand basic SQL syntax and statements\nBe able to answer basic questions about data"
  },
  {
    "objectID": "modules/week03/index-03.html#slides-and-other-materials",
    "href": "modules/week03/index-03.html#slides-and-other-materials",
    "title": "Week 3 - Structured Query Language (SQL) & DuckDB",
    "section": "Slides and other materials",
    "text": "Slides and other materials\nslides-03.pptx\nLecture notes:\n\nlecture-notes-03-mon.txt\nlecture-notes-03-wed.txt\nclass-script-03-wed.sql\n\nASDN dataset ER (entity-relationship) diagram\nClass data GitHub repository, week 3"
  },
  {
    "objectID": "modules/week03/index-03.html#resources",
    "href": "modules/week03/index-03.html#resources",
    "title": "Week 3 - Structured Query Language (SQL) & DuckDB",
    "section": "Resources",
    "text": "Resources\n\nhttp://swcarpentry.github.io/sql-novice-survey/\n\nGood Carpentry lesson, our lesson is drawn from this.\n\nC.J. Date and Hugh Darwen (1993). A Guide to the SQL Standard. 3rd ed. Reading, MA: Addison-Wesley.\nAccess via Library Catalog\n\nThe ANSI standard.\n\nJoe Celko (1995). Joe Celko’s SQL For Smarties: Advanced SQL Programming. San Francisco, CA: Morgan Kaufmann.\nAccess via Library Catalog\n\nThis guy is an SQL guru. Newer versions of this book are available online, check the Library catalog (a bug is preventing me from linking directly).\n\nGrant Allen and Mike Owens (2010). The Definitive Guide to SQLite. 2nd ed. Berkeley, CA: Apress.\nAccess via Library Catalog\n\nGood reference. Can access online!\n\nJeffrey D. Ullman and Jennifer Widom (2008). A First Course in Database Systems. 3rd ed. Upper Saddle River, NJ: Pearson/Prentice Hall.\nAccess via Library Catalog\n\nComplete but theoretical introduction to relational databases, data modeling, and relational algebra."
  },
  {
    "objectID": "modules/week03/index-03.html#homework",
    "href": "modules/week03/index-03.html#homework",
    "title": "Week 3 - Structured Query Language (SQL) & DuckDB",
    "section": "Homework",
    "text": "Homework\nSQL problem 1\nSQL problem 2\nSQL problem 3"
  },
  {
    "objectID": "modules/week04/hw-04-3.html",
    "href": "modules/week04/hw-04-3.html",
    "title": "Week 4 - Who’s the culprit?",
    "section": "",
    "text": "You’re reading up on how eggs are aged by floating them in water 1:\nwhen you receive an urgent phone call from a colleague who says they just discovered that an observer, who worked at the “nome” site between 1998 and 2008 inclusive, had been floating eggs in salt water and not freshwater. The density of salt water being different, those measurements are incorrect and need to be adjusted. The colleague says that this incorrect technique was used on exactly 36 nests, but before you can ask who the observer was, the phone is disconnected. Who made this error? That is, looking at nest data for “nome” between 1998 and 2008 inclusive, and for which egg age was determined by floating, can you determine the name of the observer who observed exactly 36 nests? Please submit your SQL. Your SQL should return exactly one row, the answer. That is, your query should produce:"
  },
  {
    "objectID": "modules/week04/hw-04-3.html#footnotes",
    "href": "modules/week04/hw-04-3.html#footnotes",
    "title": "Week 4 - Who’s the culprit?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLiebezeit, Joseph R., et al. “Assessing the Development of Shorebird Eggs Using the Flotation Method: Species-Specific and Generalized Regression Models.” The Condor, vol. 109, no. 1, 2007, pp. 32–47. JSTOR, http://www.jstor.org/stable/4122529↩︎"
  },
  {
    "objectID": "modules/week04/index-04.html",
    "href": "modules/week04/index-04.html",
    "title": "Week 4 - SQL and DuckDB",
    "section": "",
    "text": "Continued exploration of SQL concepts including joins, views, set operations, and triggers\nRead and write data from SQLite\nWrite a basic Bash script"
  },
  {
    "objectID": "modules/week04/index-04.html#learning-objectives",
    "href": "modules/week04/index-04.html#learning-objectives",
    "title": "Week 4 - SQL and DuckDB",
    "section": "",
    "text": "Continued exploration of SQL concepts including joins, views, set operations, and triggers\nRead and write data from SQLite\nWrite a basic Bash script"
  },
  {
    "objectID": "modules/week04/index-04.html#slides-and-other-materials",
    "href": "modules/week04/index-04.html#slides-and-other-materials",
    "title": "Week 4 - SQL and DuckDB",
    "section": "Slides and other materials",
    "text": "Slides and other materials\nslides-04-intro.pptx\nslides-04-triggers.pptx\nLecture notes:\n\nlecture-notes-04-mon.txt\nlecture-notes-04-wed.txt\n\nASDN dataset ER (entity-relationship) diagram\nClass data GitHub repository, week 4"
  },
  {
    "objectID": "modules/week04/index-04.html#homework",
    "href": "modules/week04/index-04.html#homework",
    "title": "Week 4 - SQL and DuckDB",
    "section": "Homework",
    "text": "Homework\nProtect yourself\nCreate a test harness"
  },
  {
    "objectID": "modules/week02/hw-02.html",
    "href": "modules/week02/hw-02.html",
    "title": "Week 2 - Data Cleaning",
    "section": "",
    "text": "Please use Canvas to return the assignments: https://ucsb.instructure.com/courses/19301/assignments/238393\nWe cleaned the Snow_cover column during class. Inspiring yourself from the steps we followed, do the following in a quarto document:\n\nClean the Water_cover column to transform it into the correct data type and respect expectations for a percentage\nClean the Land_cover column to transform it into the correct data type and respect expectations for a percentage\nUse the relationship between the three cover columns (Snow, Water, Land) to infer missing values where possible and recompute the Total_cover column\n\nAdd comments to your quarto document about your decisions and assumptions, this will be part of the grading.\n\nSetup\nYou should start a new quarto document named eds213_data_cleaning_assign_YOURNAME.qmd in your fork of the GitHub repository bren-meds213-data-cleaning.\n\n\nThe expectations are:\n\nThe qmd eds213_data_cleaning_assign_YOURNAME.qmd should run if your repo is cloned locally and\nthe code should output a csv file named all_cover_fixed_YOURNAME.csv in the data/processed folder\n\nNote: We recommend starting by importing the csv file with the corrected Snow_cover column (data/processed/snow_cover.csv) we generated during class (my code here)\n\n\nOn Canvas:\n\nAdd the URL to your fork on GitHub at the top of your quarto document\nUpload the qmd & csv files to Canvas (those versions will be the ones evaluated for grading)"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "This course will teach students the fundamentals of relational databases and data management. Students will learn the principles of database modeling and design and gain practical experience applying SQL (Structured Query Language) to manage and manipulate relational databases. The course also introduces the role and application of data documentation and metadata standards for interoperability and effective data management. By the end of the course, students will be equipped to make informed decisions about managing databases and data ethically and responsibly, focusing on issues such as bias, data privacy, sharing, ownership, and licensing."
  },
  {
    "objectID": "syllabus.html#overview",
    "href": "syllabus.html#overview",
    "title": "Course syllabus",
    "section": "",
    "text": "This course will teach students the fundamentals of relational databases and data management. Students will learn the principles of database modeling and design and gain practical experience applying SQL (Structured Query Language) to manage and manipulate relational databases. The course also introduces the role and application of data documentation and metadata standards for interoperability and effective data management. By the end of the course, students will be equipped to make informed decisions about managing databases and data ethically and responsibly, focusing on issues such as bias, data privacy, sharing, ownership, and licensing."
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Course syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nUnderstand the fundamental principles of relational databases and relational data modeling, including table structures, primary and foreign keys, relationships between tables, and data normalization.\nUnderstand how to use the Unix command line and how manage DuckDB databases from the command line.\nUse SQL to retrieve, manipulate, and manage data stored in a relational database.\nDemonstrate proficiency in querying, filtering, sorting, and programmatically accessing and interacting with relational databases from R and Python.\nBecome familiar with advanced database topics such as concurrency, transactions, indexing, backups, and publication.\nUnderstand the role of good data documentation and metadata standards for interoperability, effective data management, and reproducibility.\nOperationalize the FAIR principles into data management practices.\nProduce a metadata record in EML (Ecological Metadata Language) and apply metadata crosswalks to programmatically convert metadata schemas.\nUnderstand the ethics of sensitive data and how to de-identify sensitive data.\nEvaluate ethical and responsible data management practices, including bias, data privacy, sharing, ownership, and licensing issues."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Course syllabus",
    "section": "Schedule",
    "text": "Schedule\n\nClass: Monday & Wednesday 9:30-10:45 am (NCEAS)\nDiscussion - session 1: Thur 1-1:50PM, Bren Hall 1510\nDiscussion - session 2: Thur 2-2:50PM, Bren Hall 1510\nOffice hours: Monday 11-12 pm (NCEAS)"
  },
  {
    "objectID": "syllabus.html#modules",
    "href": "syllabus.html#modules",
    "title": "Course syllabus",
    "section": "Modules",
    "text": "Modules\n\n\n\nWeek\nTopic/Content\n\n\n\n\n1\nRelational databases and data modeling\n\n\n2\nAnalyzing & cleaning the bird dataset (from csv)\n\n\n3\nIntroduction to SQL part 1 & DuckDB\n\n\n4\nImporting data in the database + SQL part 2\n\n\n5\nAnalyzing the bird database using SQL + bash programming\n\n\n6\nUsing R & python to query databases\n\n\n7\nDocumenting your work: metdata & computing environment\n\n\n8\nSensitive data\n\n\n9\nEthical & responsible data mgnt\n\n\n10\nData licensing and publication\n\n\n\n*Schedule subject to change."
  },
  {
    "objectID": "syllabus.html#course-assessment",
    "href": "syllabus.html#course-assessment",
    "title": "Course syllabus",
    "section": "Course assessment",
    "text": "Course assessment\nYour performance in this course will depend 90% on weekly homework assignments and 10% on class participation. There will be no graded exercises or homework the last week. We will use Canvas to manage the assignments."
  },
  {
    "objectID": "syllabus.html#attendance-and-homework-policy",
    "href": "syllabus.html#attendance-and-homework-policy",
    "title": "Course syllabus",
    "section": "Attendance and homework policy",
    "text": "Attendance and homework policy\nAttendance is required. Material will be given in class that is not covered by slides or background readings.\nHomework is expected to be turned in on time. A generous amount of time will be given to complete assignments. Do not wait until the last minute to work on homework in case something unexpected comes up. Homework turned in late will be docked 20% per day."
  },
  {
    "objectID": "syllabus.html#code-of-conduct",
    "href": "syllabus.html#code-of-conduct",
    "title": "Course syllabus",
    "section": "Code of conduct",
    "text": "Code of conduct\nAll students are expected to read and comply with the UCSB Student Conduct Code. We expect cooperation from all members to help ensure a welcoming and inclusive environment for everybody. We are determined to make our courses welcoming, inclusive and harassment-free for everyone regardless of gender, gender identity and expression, race, age, sexual orientation, disability, physical appearance, or religion (or lack thereof). We do not tolerate harassment of class participants, teaching assistants, or instructors in any form. Derogatory, abusive, or demeaning language or imagery will not be tolerated."
  },
  {
    "objectID": "syllabus.html#student-support",
    "href": "syllabus.html#student-support",
    "title": "Course syllabus",
    "section": "Student support",
    "text": "Student support\nWe understand that ongoing crises impact students differently based on experiences, identities, living situations and resources, family responsibilities, and unforeseen challenges. We encourage you to prioritize your well-being. We are here to help you reach your learning and career goals. You are always welcome to reach out to our teaching team so that we can best support you. Please see the UCSB Campus Resource Guide for campus student support and services."
  },
  {
    "objectID": "syllabus.html#disabled-students-program",
    "href": "syllabus.html#disabled-students-program",
    "title": "Course syllabus",
    "section": "Disabled students program",
    "text": "Disabled students program\nStudents with disabilities and/or alternative learning needs are encouraged to work with the Disabled Students Program at UCSB to ensure we can best support your learning and success."
  },
  {
    "objectID": "modules/week02/index-02.html",
    "href": "modules/week02/index-02.html",
    "title": "Week 2 - Analyzing & cleaning the bird dataset (from csv)",
    "section": "",
    "text": "The original shorebird data set has been collected over many years by many different researchers. It is thus prone to have some data quality issues. Before we can ingest our data into our database, we will have to implement some data cleaning on the csv files to make sure we do not loose information during the import due to the constraints we can impose on the database. And in any case, the garbage in, garbage out motto often use in modeling applies here as well!\nHere is the repository where we are going to practice our data wranglers skills:\nhttps://github.com/UCSB-Library-Research-Data-Services/bren-meds213-data-cleaning"
  },
  {
    "objectID": "modules/week02/index-02.html#cleaning-the-raw-data",
    "href": "modules/week02/index-02.html#cleaning-the-raw-data",
    "title": "Week 2 - Analyzing & cleaning the bird dataset (from csv)",
    "section": "",
    "text": "The original shorebird data set has been collected over many years by many different researchers. It is thus prone to have some data quality issues. Before we can ingest our data into our database, we will have to implement some data cleaning on the csv files to make sure we do not loose information during the import due to the constraints we can impose on the database. And in any case, the garbage in, garbage out motto often use in modeling applies here as well!\nHere is the repository where we are going to practice our data wranglers skills:\nhttps://github.com/UCSB-Library-Research-Data-Services/bren-meds213-data-cleaning"
  },
  {
    "objectID": "modules/week02/index-02.html#analyzing-the-data-from-the-csv-files",
    "href": "modules/week02/index-02.html#analyzing-the-data-from-the-csv-files",
    "title": "Week 2 - Analyzing & cleaning the bird dataset (from csv)",
    "section": "Analyzing the data from the csv files",
    "text": "Analyzing the data from the csv files\nNow that we have cleaned some of the tables, let’s try to conduct some data analyses to start exploring the data set:\nhttps://github.com/UCSB-Library-Research-Data-Services/bren-meds213-data-analysis"
  },
  {
    "objectID": "modules/week04/hw-04-2.html",
    "href": "modules/week04/hw-04-2.html",
    "title": "Week 4 - Who worked with whom?",
    "section": "",
    "text": "The Camp_assignment table lists where each person worked and when. Your goal is to answer, Who work with whom? That is, find all pairs of people who worked at the same site, and whose date ranges overlap while at that site. This can be solved using a self-join.\nA self-join of a table is regular join, but instead of joining two different tables, we join two copies of the same table, which we will call the “A” copy and the “B” copy:\nThe idea is that the above join will give us rows that pair every person/site/date range with every person/site/date range. With no conditions on the join, since there are \\(441\\) rows in the Camp_assignment table, the join will produce \\(441^2 = 194481\\) rows. But out of all those rows we want only those where the two people worked at the same site. So:\nSubmit your final SQL query."
  },
  {
    "objectID": "modules/week04/hw-04-2.html#bonus",
    "href": "modules/week04/hw-04-2.html#bonus",
    "title": "Week 4 - Who worked with whom?",
    "section": "Bonus!",
    "text": "Bonus!\nProduce this much nicer table by joining with the Personnel table.\n┌─────────┬─────────────────────┬───────────────────┐\n│  Site   │       Name_1        │      Name_2       │\n│ varchar │       varchar       │      varchar      │\n├─────────┼─────────────────────┼───────────────────┤\n│ lkri    │ Anastasia Popovkina │ Gleb Sedash       │\n│ lkri    │ Anastasia Popovkina │ Julya Loshchagina │\n└─────────┴─────────────────────┴───────────────────┘\nYou’ll need to join with the Personnel table twice, once for each observer column. You may need give abbreviations to tables (e.g., JOIN Personnel AS p1). You can do it!"
  },
  {
    "objectID": "modules/week04/hw-04-1.html",
    "href": "modules/week04/hw-04-1.html",
    "title": "Week 4 - What sites are missing?",
    "section": "",
    "text": "Which sites have no egg data? Please answer this question using the three techniques demonstrated in class. In doing so, your query will involve the Bird_eggs table, the Site table, or both. As a reminder, the techniques are:\n\nUsing a Code NOT IN (subquery) clause.\nUsing an outer join with a WHERE clause that selects the desired rows. Caution: make sure your IS NULL test is performed against a column that is not ordinarily allowed to be NULL.\nUsing the set operation EXCEPT.\n\nAdd an ORDER BY clause to your queries so that all three produce the exact same result:\n┌─────────┐\n│  Code   │\n│ varchar │\n├─────────┤\n│ barr    │\n│ burn    │\n│ bylo    │\n│ cakr    │\n│ cari    │\n│ chau    │\n│ coat    │\n│ colv    │\n│ iglo    │\n│ ikpi    │\n│ lkri    │\n│ made    │\n│ nome    │\n│ prba    │\n├─────────┤\n│ 14 rows │\n└─────────┘"
  },
  {
    "objectID": "modules/week03/hw-03-2.html",
    "href": "modules/week03/hw-03-2.html",
    "title": "Week 3 - SQL problem 2",
    "section": "",
    "text": "Part 1\nIf we want to know which site has the largest area, it’s tempting to say\nSELECT Site_name, MAX(Area) FROM Site;\nWouldn’t that be great? But DuckDB gives an error. And right it should! This query is conceptually flawed. Please describe what is wrong with this query. Don’t just quote DuckDB’s error message— explain why DuckDB is objecting to performing this query.\nTo help you answer this question, you may want to consider:\n\nTo the database, the above query is no different from\n\nSELECT Site_name, AVG(Area) FROM Site\nSELECT Site_name, COUNT(*) FROM Site\nSELECT Site_name, SUM(Area) FROM Site\n\nIn all these examples, the database sees that it is being asked to apply an aggregate function to a table column.\nWhen performing an aggregation, SQL wants to collapse the requested columns down to a single row. (For a table-level aggregation such as requested above, it wants to collapse the entire table down to a single row. For a GROUP BY, it wants to collapse each group down to a single row.)\n\n\n\nPart 2\nTime for plan B. Find the site name and area of the site having the largest area. Do so by ordering the rows in a particularly convenient order, and using LIMIT to select just the first row. Your result should look like:\n┌──────────────┬────────┐\n│  Site_name   │  Area  │\n│   varchar    │ float  │\n├──────────────┼────────┤\n│ Coats Island │ 1239.1 │\n└──────────────┴────────┘\nPlease submit your SQL.\n\n\nPart 3\nDo the same, but use a nested query. First, create a query that finds the maximum area. Then, create a query that selects the site name and area of the site whose area equals the maximum. Your overall query will look something like:\nSELECT Site_name, Area FROM Site WHERE Area = (SELECT ...);"
  },
  {
    "objectID": "modules/week03/hw-03-3.html",
    "href": "modules/week03/hw-03-3.html",
    "title": "Week 3 - SQL problem 3",
    "section": "",
    "text": "Your mission is to list the scientific names of bird species in descending order of their maximum average egg volumes. That is, compute the average volume of the eggs in each nest, and then for the nests of each species compute the maximum of those average volumes, and list by species in descending order of maximum volume. You final table should look like:\n┌─────────────────────────┬────────────────────┐\n│     Scientific_name     │   Max_avg_volume   │\n│         varchar         │       double       │\n├─────────────────────────┼────────────────────┤\n│ Pluvialis squatarola    │   36541.8525390625 │\n│ Pluvialis dominica      │    33847.853515625 │\n│ Arenaria interpres      │   23338.6220703125 │\n│ Calidris fuscicollis    │ 13277.143310546875 │\n│ Calidris alpina         │ 12196.237548828125 │\n│ Charadrius semipalmatus │ 11266.974975585938 │\n│ Phalaropus fulicarius   │  8906.775146484375 │\n└─────────────────────────┴────────────────────┘\n(By the way, regarding the leader in egg size above, Birds of the World says that Pluvialis squatarola’s eggs are “Exceptionally large for size of female (ca. 16% weight of female)”.)\nTo calculate the volume of an egg, use the simplified formula\n\\[{\\pi \\over 6} W^2 L\\]\nwhere \\(W\\) is the egg width and \\(L\\) is the egg length. You can use 3.14 for \\(\\pi\\). (The real formula takes into account the ovoid shape of eggs, but only width and length are available to us here.)\nA good place to start is just to group bird eggs by nest (i.e., Nest_ID) and compute average volumes:\nCREATE TEMP TABLE Averages AS\n    SELECT Nest_ID, AVG(...) AS Avg_volume\n        FROM ...\n        GROUP BY ...;\nYou can now join that table with Bird_nests, so that you can group by species, and also join with the Species table to pick up scientific names. To do just the first of those joins, you could say something like\nSELECT Species, MAX(...)\n    FROM Bird_nests JOIN Averages USING (Nest_ID)\n    GROUP BY ...;\n(Notice how, if the joined columns have the same name, you can more compactly say USING (common_column) instead of ON column_a = column_b.)\nThat’s not the whole story, we want scientific names not species codes. Another join is needed. A couple strategies here. One, you can modify the above query to also join with the Species table (you’ll need to replace USING with ON …). Two, you can save the above as another temp table and join it to Species separately.\nDon’t forget to order the results. Here it is convenient to give computed quantities nice names so you can refer to them.\nPlease submit all of the SQL you used to solve the problem. Bonus points if you can do all of the above in one statement."
  },
  {
    "objectID": "modules/week07/index-07.html",
    "href": "modules/week07/index-07.html",
    "title": "Week 7 - Metadata standards",
    "section": "",
    "text": "Understand the role of metadata in facilitating the management, discovery, interoperability, and reuse of digital resources and research data.\nRecognize the importance of adhering to domain-specific metadata standards and determining which standard should be used.\nAcquire familiarity with the overall structure of the Ecological Metadata Language (EML) schema and the anatomy of an EML record.\nProgrammatically create EML metadata records using R."
  },
  {
    "objectID": "modules/week07/index-07.html#learning-goals",
    "href": "modules/week07/index-07.html#learning-goals",
    "title": "Week 7 - Metadata standards",
    "section": "",
    "text": "Understand the role of metadata in facilitating the management, discovery, interoperability, and reuse of digital resources and research data.\nRecognize the importance of adhering to domain-specific metadata standards and determining which standard should be used.\nAcquire familiarity with the overall structure of the Ecological Metadata Language (EML) schema and the anatomy of an EML record.\nProgrammatically create EML metadata records using R."
  },
  {
    "objectID": "modules/week07/index-07.html#slides-and-other-materials",
    "href": "modules/week07/index-07.html#slides-and-other-materials",
    "title": "Week 7 - Metadata standards",
    "section": "Slides and other materials",
    "text": "Slides and other materials\nslides-07.pptx\nClass data GitHub repository, week 6 for in-class exercise\nRelated resources:\n\nResearch Data Alliance (RDA) Metadata Standard Catalog.\nEML Schema Documentation.\nEML Best Practices.\nCreating EML records in R.\nEzEML - Ecological Data Initiative (EDI).\nExcel to EML.\ncapemlGIS - a package that extends the CAPLTER/capeml to facilitate the creation of EML spatialRaster and spatialVector objects and metadata."
  },
  {
    "objectID": "modules/week07/index-07.html#recommended-readings",
    "href": "modules/week07/index-07.html#recommended-readings",
    "title": "Week 7 - Metadata standards",
    "section": "Recommended readings",
    "text": "Recommended readings\n\nGries, C., Hanson, P. C., O’Brien, M., Servilla, M., Vanderbilt, K., & Waide, R. (2023). The Environmental Data Initiative: Connecting the past to the future through data reuse. Ecology and Evolution, 13(1), e9592. https://doi.org/10.1002/ece3.9592\nLeipzig, J., Nüst, D., Hoyt, C. T., Ram, K., & Greenberg, J. (2021). The role of metadata in reproducible computational research. Patterns, 2(9), https://doi.org/10.1016/j.patter.2021.100322"
  },
  {
    "objectID": "modules/week07/index-07.html#homework",
    "href": "modules/week07/index-07.html#homework",
    "title": "Week 7 - Metadata standards",
    "section": "Homework",
    "text": "Homework\nEML record"
  },
  {
    "objectID": "modules/week06/index-06.html",
    "href": "modules/week06/index-06.html",
    "title": "Week 6 - Programming with databases",
    "section": "",
    "text": "Review: understand how the data model relates to queries\nUnderstand the basic database programming model\nAccess an SQLite database from Python and R\nUnderstand how to use the Python/Pandas and R/dbplyr convenience functions"
  },
  {
    "objectID": "modules/week06/index-06.html#learning-objectives",
    "href": "modules/week06/index-06.html#learning-objectives",
    "title": "Week 6 - Programming with databases",
    "section": "",
    "text": "Review: understand how the data model relates to queries\nUnderstand the basic database programming model\nAccess an SQLite database from Python and R\nUnderstand how to use the Python/Pandas and R/dbplyr convenience functions"
  },
  {
    "objectID": "modules/week06/index-06.html#slides-and-other-materials",
    "href": "modules/week06/index-06.html#slides-and-other-materials",
    "title": "Week 6 - Programming with databases",
    "section": "Slides and other materials",
    "text": "Slides and other materials\nslides-06.pptx\nCoding transcripts:\n\ntranscript-06-mon.ipynb\ntranscript-06-wed-python.ipynb\ntranscript-06-wed-r.R"
  },
  {
    "objectID": "modules/week06/index-06.html#resources",
    "href": "modules/week06/index-06.html#resources",
    "title": "Week 6 - Programming with databases",
    "section": "Resources",
    "text": "Resources\n\nhttps://peps.python.org/pep-0249/\n\nCommon Python-RDBMS API\n\nhttps://duckdb.org/docs/api/python/overview\n\nPython DuckDB module\n\nhttps://dbplyr.tidyverse.org/\n\nR dbplyr documentation"
  },
  {
    "objectID": "modules/week06/index-06.html#homework",
    "href": "modules/week06/index-06.html#homework",
    "title": "Week 6 - Programming with databases",
    "section": "Homework",
    "text": "Homework\nLittle Bobby Tables\nWho’s the culprit?\nCharacterizing egg variation\nWho were the winners?"
  },
  {
    "objectID": "modules/week06/hw-06-3.html",
    "href": "modules/week06/hw-06-3.html",
    "title": "Week 6 - Characterizing egg variation",
    "section": "",
    "text": "You read Egg Dimensions and Neonatal Mass of Shorebirds by Robert E. Ricklefs and want to see how the egg data we’ve been using in class compares to his results. Specifically, Ricklefs reported, “Coefficients of variation were 4 to 9% for egg volume” for shorebird eggs gathered in Manitoba, Canada. What is the range of coefficients of variation in our ASDN dataset?\nThe “coefficient of variation,” or CV, is a unitless measure of the variation of a sample, defined as the standard deviation divided by the mean and multiplied by 100 to express as a percentage. Thus, a CV of 10% means the standard deviation is 10% of the mean value. For the purposes of this computation, we will copy Ricklefs and use as a proxy for egg volume the formula\n\\[ W^2 L \\]\nwhere \\(W\\) is egg width and \\(L\\) is egg length.\nYour task is to create a Python program that reads data from the ASDN database and uses Pandas to compute, for each species in the database (for which there is egg data), the coefficient of variation of volume using the above formula. There are many ways this can be done. Because this assignment is primarily about programming in Python, please follow the steps below. Please submit your Python code when done.\n\nStep 1\nCreate a query that will return the distinct species for which there is egg data (not all species and not all nests have egg data), so that you can then loop over those species. Your query should return two columns, species code and scientific name.\n\n\nStep 2\nAfter you’ve connected to the database and created a cursor c, iterate over the species like so:\nspecies_query = \"\"\"SELECT Code, Scientific_name FROM...\"\"\"\nfor row in c.execute(species_query):\n    species_code = row[0]\n    scientific_name = row[1]\n    # query egg data for that species (step 3)\n    # compute statistics and print results (step 4)\n\n\nStep 3\nYou will need to construct a query template that gathers egg data for a species that will be supplied as a parameter. You can compute the formula\n\\[ W^2 L \\]\nin SQL or in Pandas. For simplicity, SQL is suggested:\negg_query = \"\"\"SELECT Width*Width*Length AS Volume FROM...\"\"\"\nWithin the loop, you will want to execute the query on the current species in the loop iteration. You may use the Pandas function pd.read_sql to do so and also directly load the results into a dataframe:\ndf = pd.read_sql(egg_query, conn, ...)\nDo a help(pd.read_sql) to figure out how to pass parameters to a query.\n\n\nStep 4\nFinally, and still within your loop, you’ll want to compute statistics and print out the results:\ncv = round(df.Volume.std()/df.Volume.mean()*100, 2)\nprint(f\"{scientific_name} {cv}%\")\nYour output should look like this:\nCharadrius semipalmatus 8.99%\nPluvialis dominica 19.88%\nPluvialis squatarola 6.94%\nCalidris alpina 5.46%\nCalidris fuscicollis 16.77%\nPhalaropus fulicarius 4.65%\nArenaria interpres 21.12%\n\n\nAppendix\nIt’s not necessary to use pd.read_sql to get data into a dataframe, it’s just a convenience. To do so manually (and to show you it’s not that hard), imagine that your query returns three columns. Collect the row data into three separate lists, then manually create a dataframe specifying the contents as a dictionary:\nrows = c.execute(\"SELECT Species, Width, Length FROM...\").fetchall()\nspecies_column = [t[0] for t in rows]\nwidth_column = [t[1] for t in rows]\nlength_column = [t[2] for t in rows]\n\ndf = pd.DataFrame(\n    {\n        \"species\": species_column,\n        \"width\": width_column,\n        \"length\": length_column\n    }\n)"
  },
  {
    "objectID": "modules/week10/index-10.html",
    "href": "modules/week10/index-10.html",
    "title": "Week 10 - Data licensing and publication",
    "section": "",
    "text": "Become familiar with data licensing in the academic context.\nDistinguish which research deliverables can or cannot be subject to copyright and explore alternatives to that.\nGain an understanding of the Creative Commons license family and the differences in their applicability.\n\n\n\n\nslides-10-part1.pptx"
  },
  {
    "objectID": "modules/week10/index-10.html#part-i---data-licensing",
    "href": "modules/week10/index-10.html#part-i---data-licensing",
    "title": "Week 10 - Data licensing and publication",
    "section": "",
    "text": "Become familiar with data licensing in the academic context.\nDistinguish which research deliverables can or cannot be subject to copyright and explore alternatives to that.\nGain an understanding of the Creative Commons license family and the differences in their applicability.\n\n\n\n\nslides-10-part1.pptx"
  },
  {
    "objectID": "modules/week10/index-10.html#part-ii---data-publication",
    "href": "modules/week10/index-10.html#part-ii---data-publication",
    "title": "Week 10 - Data licensing and publication",
    "section": "Part II - Data publication",
    "text": "Part II - Data publication\n\nLearning goals\n\nUnderstand the importance of publishing research data.\nIdentify and select appropriate approaches to data publication.\nExplain the role of persistent identifiers.\n\n\n\nSlides\nslides-10-part2.pptx"
  },
  {
    "objectID": "modules/week10/index-10.html#suggested-readings",
    "href": "modules/week10/index-10.html#suggested-readings",
    "title": "Week 10 - Data licensing and publication",
    "section": "Suggested readings*",
    "text": "Suggested readings*\n\nCarroll, M. W. (2015) Sharing Research Data and Intellectual Property Law: A Primer. PLoS Biol 13(8): e1002235. https://doi.org/10.1371/journal.pbio.1002235\nFay, C. (2019). Licensing R. https://thinkr-open.github.io/licensing-r\nReitz, K., & Schlusser, T. (2022). The Hitchhiker’s guide to Python: best practices for development. ” O’Reilly Media, Inc.”. https://docs.python-guide.org/writing/license\n\n*Useful links and other supporting materials are noted in the slides."
  },
  {
    "objectID": "modules/week10/index-10.html#homework",
    "href": "modules/week10/index-10.html#homework",
    "title": "Week 10 - Data licensing and publication",
    "section": "Homework",
    "text": "Homework\nNo homework this week!"
  },
  {
    "objectID": "modules/week08/hw-08.html",
    "href": "modules/week08/hw-08.html",
    "title": "Week 8 - sdcMicro exercise",
    "section": "",
    "text": "Your team has successfully obtained a dataset1 that encompasses whale entanglement data associated with specific fisheries along the West Coast. This dataset, named whale-sdc.csv, and an accompanying file called whale-exercise.Rmd, are available on the class data GitHub repository, week 8 directory.\nIn groups of two or three, your task is to thoroughly examine the dataset and complete the provided R Markdown file. This entails implementing the necessary code and addressing the given questions. To ensure proper identification, please include the names of all participating members in the YAML header before submitting the modified R Markdown file."
  },
  {
    "objectID": "modules/week08/hw-08.html#footnotes",
    "href": "modules/week08/hw-08.html#footnotes",
    "title": "Week 8 - sdcMicro exercise",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis dataset was purposefully adapted exclusively for instruction use.↩︎"
  },
  {
    "objectID": "modules/week05/hw-04-1.html",
    "href": "modules/week05/hw-04-1.html",
    "title": "Week 4 - Protect yourself",
    "section": "",
    "text": "Some example strategies were mentioned in class for reducing the possibility of performing UPDATEs and DELETEs that have catastrophic consequences. What strategy will you use?"
  },
  {
    "objectID": "modules/week05/hw-05-2.html",
    "href": "modules/week05/hw-05-2.html",
    "title": "Week 5 - What makes a good index?",
    "section": "",
    "text": "Recall from class that an index IC on a column C in a table T is in effect a mini-table, kept in sync with T, that contains all the values of column C in order. If there are a million rows in table T, there will be a million values in index IC. If the values of column C are unique, the index will hold a million unique values. If column C takes on only a few possible values, then index IC will still have a million values, but many of those values will be repeated.\nSuppose we are given a query that includes a constraint against column C, i.e., that includes WHERE C = someval possibly among other constraints. To use index IC means that the database looks up the constraint value someval in the index to obtain a smaller number of table rows (in the ideal case, just one row in the case of a unique index) to subsequently examine and match additional constraints against. The essential purpose of an index is to reduce the number of table rows that must be examined.\nFor the purposes of this assignment we will be working with a database that has the same structure as previous class databases but in which the Bird_nests table has 1 million rows, each of which has been fattened to occupy multiple kilobytes. As a consequence, the database file is approximately 4GB. The database file can be found on taylor.bren.ucsb.edu at /courses/EDS213/database-long-wide.db. To use it you will have to copy or download it to your own space.\nWe will also be working with the following query:\nSELECT Nest_ID\n    FROM Bird_nests\n    WHERE Site = 'nome' AND\n          Species = 'ruff' AND\n          Year = 1983 AND\n          Observer = 'cbishop' AND\n          ageMethod = 'float';\nThis query returns exactly one row.\n\nPart 1\nAnswer the following questions.\n\nIs there already an index on the Bird_nests table? If so, what is that index and will SQLite use it in the above query? Why or why not?\nWill adding an index on a column not mentioned in the WHERE clause be used by the database? Why or why not?\n\n\n\nPart 2\nQuery optimization, which falls under the general heading of “database tuning,” is a complex subject, as query performance depends on the query or queries being supported, the distribution and nature of the data, the abilities and characteristics of the query planner, and the costs of creating and maintaining indexes. Still, we can make a general observation about what makes for a good index (“good” meaning improving query performance here) by examining the effects of adding different indexes on the Bird_nests table and timing the above query.\nYour task is to conduct at least 10 experiments. In each experiment perform the following steps:\n\nCreate an index on a column or columns.\nTime the above query.\nDrop the index (to avoid accumulating indexes on the table).\nAlso, determine the number of distinct values in the index, i.e., the number of distinct values in the column or number of distinct tuples if the index is over multiple columns.\n\nWhat experiments to run?\n\nRun an initial experiment with no added index. For this experiment only, record the number of distinct values as 1.\nTry adding an index on each column mentioned in the WHERE clause (e.g., Site): that’s 5 experiments right there.\nFor other experiments, try adding an index on multiple columns, e.g., Site and Observer, or even 3 or 4 columns together.\n\nWhen done, you will want your results in the form of a CSV file with three columns: label, query time, and number of distinct values.\nFor timing, you will probably want to use the Bash test harness you developed in week 5. You can run your test harness manually, and manually retrieve the number of distinct values for step 4 above. But you can also automate the whole process; some hints for doing so are below. Be careful when running your test harness, as in the end you want just one row in your CSV file per experiment.\nOnce you have your data, load it in Jupyter or RStudio and create a log-log scatter plot of the number of distinct index values (X axis) versus query time (Y axis). (Results will be difficult to see if both axes are not logarithmic.) What relationship do you observe? Please hypothesize why you see the relationship you do.\nPlease upload your Jupyter notebook or Rmarkdown document. You do not need to submit your data.\n\n\nModifying your test harness\nA few tips on modifying your test harness to make it more useful for this assignment. First, if you find it annoying to have to try different numbers of repetitions to get positive and more precise timings, you can automate your script to try different numbers of repetitions until it achieves something reasonable. Here’s one idea:\nnum_reps=1\nwhile true; do\n    start=$SECONDS\n    for _ in $(seq $num_reps); do\n        sqlite3 $db_file \"$query\" &gt;& /dev/null\n    done\n    end=$SECONDS\n    if [ $((end - start)) -gt 3 ]; then\n        break\n    fi\n    echo \"Too fast, trying again!\"\n    num_reps=$((num_reps * 10))\ndone\nThis will try 1 repetition, then 10, then 100, etc.\nSecond, you can gather the number of distinct values by adding an sqlite3 -csv $db_file \"$num_values_query\" command and saving the output in a variable, and then echoing that variable out to your CSV file along with your other variables.\nAnd third, if you make the column(s) you want to index an argument to your script, you can automate adding and dropping indexes by adding more sqlite3 commands."
  },
  {
    "objectID": "modules/week05/indexes/hw-05-1.html",
    "href": "modules/week05/indexes/hw-05-1.html",
    "title": "Week 4 - Bash scripting",
    "section": "",
    "text": "Read Ten Bash Essentials and answer the 8 questions at the end. Be sure you are in the week5 directory of the class data GitHub repository before answering the questions, as they rely on the files contained therein."
  },
  {
    "objectID": "modules/week01/hw-01-2.html",
    "href": "modules/week01/hw-01-2.html",
    "title": "Week 1 - Data modeling",
    "section": "",
    "text": "Please use Canvas to return the assignments: https://ucsb.instructure.com/courses/19301/assignments/224311\nCreate a table definition for the Snow_survey table that is maximally expressive, that is, that captures as much of the semantics and characteristics of the data using SQL’s data definition language as is possible.\nIn the class data GitHub repository, week 1 directory you will find the table described in the metadata (consult 01_ASDN_Readme.txt) and the data can be found in ASDN_Snow_survey.csv. You will want to look at the values that occur in the data using a tool like R, Python, or OpenRefine.\nPlease consider:\nYou may (or may not) want to take advantage of the Species, Site, Color_band_code, and Personnel supporting tables. These are also documented in the metadata, and SQL table definitions for them have already been created and are included below.\nPlease express your table definition in SQL, but don’t worry about getting the SQL syntax exactly correct. This assignment is just a thought exercise. If you do want to try to write correct SQL, though, your may find it helpful to consult the DuckDB CREATE TABLE documentation.\nFinally, please provide some explanation for why you made the choices you did, and any questions or uncertainties you have. Don’t write an essay! Bullet points are sufficient. But do please explain your thought process."
  },
  {
    "objectID": "modules/week01/hw-01-2.html#appendix",
    "href": "modules/week01/hw-01-2.html#appendix",
    "title": "Week 1 - Data modeling",
    "section": "Appendix",
    "text": "Appendix\nCREATE TABLE Species (\n    Code TEXT PRIMARY KEY,\n    Common_name TEXT UNIQUE NOT NULL,\n    Scientific_name TEXT,\n    Relevance TEXT\n);\n\nCREATE TABLE Site (\n    Code TEXT PRIMARY KEY,\n    Site_name TEXT UNIQUE NOT NULL,\n    Location TEXT NOT NULL,\n    Latitude REAL NOT NULL CHECK (Latitude BETWEEN -90 AND 90),\n    Longitude REAL NOT NULL CHECK (Longitude BETWEEN -180 AND 180),\n    \"Total_Study_Plot_Area_(ha)\" REAL NOT NULL\n        CHECK (\"Total_Study_Plot_Area_(ha)\" &gt; 0),\n    UNIQUE (Latitude, Longitude)\n);\n\nCREATE TABLE Color_band_code (\n    Code TEXT PRIMARY KEY,\n    Color TEXT NOT NULL UNIQUE\n);\n\nCREATE TABLE Personnel (\n    Abbreviation TEXT PRIMARY KEY,\n    Name TEXT NOT NULL UNIQUE\n);"
  },
  {
    "objectID": "modules/week01/hw-01-1.html",
    "href": "modules/week01/hw-01-1.html",
    "title": "Week 1 - Create an ER diagram",
    "section": "",
    "text": "Please use Canvas to return the assignments: https://ucsb.instructure.com/courses/19301/assignments/236835\nCreate a physical ER (entity-relationship) diagram for the Harry Potter tables shown in class. It will be helpful to refer back to the slides.\nAs discussed briefly in class, a logical or conceptual ER diagram focuses on high-level abstractions, and doesn’t address how entities and relationships actually get implemented. In particular, in a logical ER diagram a many-to-many relationship between two entities might be represented by a simple line, even though in implementation a many-to-many relationship requires a separate table to store the relationship tuples. By contrast, a physical ER diagram describes actual tables. You are being asked to create a physical ER diagram.\nRequirements:\n\nYour diagram should include Student, House, Wand, Course, and Enrollment tables.\nEach table should list the name of the entity, any attributes, and which attribute(s) form the primary key, if there is one.\nA foreign key relationship from an attribute in one table to an attribute in another table should be indicated by a line between the two attributes. The ends of the lines should reflect the cardinalities at each end. See the example below.\n\nTwist #1! The slides shown in class demonstrated a many-to-one relationship between wands and students, i.e., one student might own multiple wands, but any given wand has only one owner. However, for this exercise, you are being asked to model a many-to-many relationship between wands and students (it happened in the books that the same wand was used by different students, though at different times, of course). To create a many-to-many relationship, you will need to invent an intermediate table that represents the student-wand ownership relation, in the same way the Enrollment table intermediates between the Student and Course tables.\nTwist #2! You must also store the date range (i.e., begin date and end date) of wand ownership. You will need to think where these date attributes belong. Are they attributes of a student? Of a wand? Of something else?\nVarious symbologies have been developed for ER diagrams. For this assignment, represent the “one” side of a many-to-one relationship by a single vertical bar, and represent the “many” side by a so-called crow’s foot. In the end, your diagram should visually resemble something like this:\n\nYou can use a tool like dbdiagram.io as was used to create the above diagram, or any other drawing tool. Or you can just draw it by hand and take a picture with your phone. Regardless of the method, be sure to indicate primary keys somehow (bold text, underlined text, add “PK” next to the attribute, etc., whatever works visually)."
  },
  {
    "objectID": "modules/week09/case-c.html",
    "href": "modules/week09/case-c.html",
    "title": "Case Study C: To reuse or not reuse, that is the key question!",
    "section": "",
    "text": "Read the scenario and answer the questions based on the weekly readings and the lecture:\nAdam is a researcher for a non-profit organization dedicated to accelerating the adoption of solar energy. The organization relies on data from various sources, including sensors, satellite imagery, and field measurements, to inform solar energy allocation, usage, and conservation decisions.\nRecently, he identified an available dataset containing data on the solar energy market size, including trends, competition, and customer demand. These data can inform business and policy decisions related to solar energy adoption. Adam is particularly excited because this is a multivariate time series dataset from the past ten years. Also, the data documentation listed many important variables for his project, including the compound annual growth rates (CAGR) for solar energy companies. However, when Adam inspected some of the data files, he noticed a few data points that needed to be corrected. For example, some rows had NAs; others had 000, 999, and -999 or were utterly blank; the documentation does not help him infer those values.\nWhen he contacted the corresponding researcher for clarification, he was told these inconsistencies could have been caused either due to system migration or by human error in inaccurate data entry. The researcher mentioned that his team had multiple contributors throughout the years and noted there were no enforced validation rules or data quality checks. Ultimately, Adam should choose a solution that balances the benefits of using the existing dataset with the potential risks of using incomplete or inaccurate data.\nAdam faces a dilemma. On the one hand, the dataset could provide valuable insights into the solar energy market and inform better policies and management decisions. On the other hand, the missing and anomalous data could affect the dataset’s overall quality and integrity, potentially leading to incorrect conclusions and decisions."
  },
  {
    "objectID": "modules/week09/case-c.html#instructions",
    "href": "modules/week09/case-c.html#instructions",
    "title": "Case Study C: To reuse or not reuse, that is the key question!",
    "section": "",
    "text": "Read the scenario and answer the questions based on the weekly readings and the lecture:\nAdam is a researcher for a non-profit organization dedicated to accelerating the adoption of solar energy. The organization relies on data from various sources, including sensors, satellite imagery, and field measurements, to inform solar energy allocation, usage, and conservation decisions.\nRecently, he identified an available dataset containing data on the solar energy market size, including trends, competition, and customer demand. These data can inform business and policy decisions related to solar energy adoption. Adam is particularly excited because this is a multivariate time series dataset from the past ten years. Also, the data documentation listed many important variables for his project, including the compound annual growth rates (CAGR) for solar energy companies. However, when Adam inspected some of the data files, he noticed a few data points that needed to be corrected. For example, some rows had NAs; others had 000, 999, and -999 or were utterly blank; the documentation does not help him infer those values.\nWhen he contacted the corresponding researcher for clarification, he was told these inconsistencies could have been caused either due to system migration or by human error in inaccurate data entry. The researcher mentioned that his team had multiple contributors throughout the years and noted there were no enforced validation rules or data quality checks. Ultimately, Adam should choose a solution that balances the benefits of using the existing dataset with the potential risks of using incomplete or inaccurate data.\nAdam faces a dilemma. On the one hand, the dataset could provide valuable insights into the solar energy market and inform better policies and management decisions. On the other hand, the missing and anomalous data could affect the dataset’s overall quality and integrity, potentially leading to incorrect conclusions and decisions."
  },
  {
    "objectID": "modules/week09/case-c.html#questions",
    "href": "modules/week09/case-c.html#questions",
    "title": "Case Study C: To reuse or not reuse, that is the key question!",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nSuppose Adam is leaning toward reusing the dataset despite the identified problems. What general ethical and responsible steps would you advise him to take moving forward? (Select all that apply)\n\nAdam should carefully examine the dataset to map all existing issues to determine the extent of missing and anomalous data and how it could affect the accuracy of his analysis.\nIf Adam collects new data, he should enforce data validation rules to tables and perform quality checks to avoid similar problems.\nIf Adam integrates new data and perform all necessarily adjustments to remove uncertainties and other problems in the data, he no longer needs to attribute original data creators.\nIf the dataset has significant missing or anomalous data, Adam may need to collect additional data to ensure the analysis is accurate and representative. This may involve additional time and resources but could lead to more accurate and reliable insights.\nAdam may be able to proceed with the analysis after carefully documenting and accounting for any uncertainties or limitations in the data.\nAdam should produce new documentation for the dataset based on all improvements he makes to the original data."
  },
  {
    "objectID": "modules/week09/case-b.html",
    "href": "modules/week09/case-b.html",
    "title": "Case Study B: The caveat of the caviar: navigating ethics to protect endangered river wildlife",
    "section": "",
    "text": "Read the scenario and answer questions based on the weekly readings and the lecture:\nA team of environmental scientists from the University of Wisconsin is researching the impact of climate change on the Mississippi River wildlife. The research involves collecting data on various environmental factors, including water temperature, salinity, pH, and nutrient levels, as well as data on the abundance and diversity of aquatic species on the lower basins of the river.\nThe team collects the data using various methods, including river sensors, underwater cameras, and traditional sampling techniques. The data is stored on a cloud-based platform, allowing sharing and real-time collaboration with multiple partners. However, the researchers soon realized that some of the collected data might be sensitive, including information on the distribution and abundance of the pallid sturgeon, recently listed by the Wisconsin Department of Natural Resources as one of the endangered species. They also realized that some data points might be of commercial interest, which can threaten the protection of the species and its habitat. For example, exposing the exact locations of the pallid sturgeon and their critical habitats could lead to increased poaching or unauthorized fishing, as the species is highly valued for its caviar.\nResearchers face ethical considerations related to sensitive data sharing. They want to comply with best practices in open science. However, in this process, they must balance the need to share their data openly to advance research and understanding of climate change and the Mississippi River ecosystem while protecting sensitive data and preventing it from being used for commercial purposes that could harm the environment."
  },
  {
    "objectID": "modules/week09/case-b.html#instructions",
    "href": "modules/week09/case-b.html#instructions",
    "title": "Case Study B: The caveat of the caviar: navigating ethics to protect endangered river wildlife",
    "section": "",
    "text": "Read the scenario and answer questions based on the weekly readings and the lecture:\nA team of environmental scientists from the University of Wisconsin is researching the impact of climate change on the Mississippi River wildlife. The research involves collecting data on various environmental factors, including water temperature, salinity, pH, and nutrient levels, as well as data on the abundance and diversity of aquatic species on the lower basins of the river.\nThe team collects the data using various methods, including river sensors, underwater cameras, and traditional sampling techniques. The data is stored on a cloud-based platform, allowing sharing and real-time collaboration with multiple partners. However, the researchers soon realized that some of the collected data might be sensitive, including information on the distribution and abundance of the pallid sturgeon, recently listed by the Wisconsin Department of Natural Resources as one of the endangered species. They also realized that some data points might be of commercial interest, which can threaten the protection of the species and its habitat. For example, exposing the exact locations of the pallid sturgeon and their critical habitats could lead to increased poaching or unauthorized fishing, as the species is highly valued for its caviar.\nResearchers face ethical considerations related to sensitive data sharing. They want to comply with best practices in open science. However, in this process, they must balance the need to share their data openly to advance research and understanding of climate change and the Mississippi River ecosystem while protecting sensitive data and preventing it from being used for commercial purposes that could harm the environment."
  },
  {
    "objectID": "modules/week09/case-b.html#questions",
    "href": "modules/week09/case-b.html#questions",
    "title": "Case Study B: The caveat of the caviar: navigating ethics to protect endangered river wildlife",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nAs a data manager, what recommendations would you offer to the researchers to avoid commercial exploitation of the pallid sturgeon while contributing to advancing the research in the field? You do not need to write a long essay; elaborating your advice in bullet points is enough.\n\n\nQuestion 2\nAs a general rule, the _____________ of endangered, sensitive species should not be shared publicly. (hint: 1-2 words)"
  },
  {
    "objectID": "installing-duckdb.html",
    "href": "installing-duckdb.html",
    "title": "Installing duckDB",
    "section": "",
    "text": "DuckDB has been installed on the MEDS server. We also recommend to install it on your personal machine following those instructions: https://duckdb.org/docs/installation/"
  },
  {
    "objectID": "installing-duckdb.html#installation",
    "href": "installing-duckdb.html#installation",
    "title": "Installing duckDB",
    "section": "",
    "text": "DuckDB has been installed on the MEDS server. We also recommend to install it on your personal machine following those instructions: https://duckdb.org/docs/installation/"
  },
  {
    "objectID": "installing-duckdb.html#visual-code-integration-optional",
    "href": "installing-duckdb.html#visual-code-integration-optional",
    "title": "Installing duckDB",
    "section": "Visual code integration (optional)",
    "text": "Visual code integration (optional)\nYou can use DuckDB directly from the terminal but if you also want to have the option to have both a SQL script and the terminal open, we recommend to use visual code. There is one setting to be done to link the script and the terminal:\nIn Visual Code:\n\nopen the palette Shift+Cmd+P and search for: Preferences: Open Keyboard Shortcuts (JSON) \nenter the following text & save\n\n// Place your key bindings in this file to override the defaults\n[\n    {\n        \"key\": \"shift+enter\",\n        \"command\": \"workbench.action.terminal.runSelectedText\"\n    }\n]\nOR alternative method to set this up:\nUnder the main VS menu, go to Settings -&gt; Keyboard Shortcuts, search for “run selected”, the “run selected text in active terminal” will be one of the options. Set the shift-enter keypress by double-clicking. Right-click on the When column, select Change When Expression, and add editorLangId == 'sql' to restrict to just SQL files and not every kind of file.\nNow you can hit Shift+Return at then end of a line in your SQL script and it should run the command directly in the terminal!"
  },
  {
    "objectID": "installing-duckdb.html#test",
    "href": "installing-duckdb.html#test",
    "title": "Installing duckDB",
    "section": "Test",
    "text": "Test\nLet’s test our new installation using visual code\n\nopen a terminal from the terminal menu -&gt; new terminal\nIn the terminal type duckdb, this should start duckdb\nOpen a New text file: file menu -&gt; new text file\nCopy paste the following code:\n\n-- Start the DB at the terminal: duckdb\n\nCREATE TABLE ducks AS SELECT 3 As age, 'mandarin' AS breed;\n\nSHOW TABLES;\n\nFROM ducks SELECT *;\n\nUse Shift+Return to run the SQL code line by line\n\nYou should have something that looks like this:"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDS 213: Databases and Data Management",
    "section": "",
    "text": "The goals of the course were to give MEDS students the skills they need to practically, successfully, and ethically manage their data, and to create, manage, and use relational databases where appropriate. Relational database topics went farther than just SQL queries and included a significant unit on data modeling and database constraints and integrity, in addition to advanced database topics such as indexes and accessing databases from programming environments. The data management portion includes analyzing data from an ethical perspective, creating standards-compliant metadata, and employing data de-identification techniques. The course also included a unit on the Unix command line, with an emphasis on creating reusable Bash scripts, given in the spirit that Bash is a generally useful tool that all data scientists should have at least some familiarity with."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "EDS 213: Databases and Data Management",
    "section": "",
    "text": "The goals of the course were to give MEDS students the skills they need to practically, successfully, and ethically manage their data, and to create, manage, and use relational databases where appropriate. Relational database topics went farther than just SQL queries and included a significant unit on data modeling and database constraints and integrity, in addition to advanced database topics such as indexes and accessing databases from programming environments. The data management portion includes analyzing data from an ethical perspective, creating standards-compliant metadata, and employing data de-identification techniques. The course also included a unit on the Unix command line, with an emphasis on creating reusable Bash scripts, given in the spirit that Bash is a generally useful tool that all data scientists should have at least some familiarity with."
  },
  {
    "objectID": "index.html#data",
    "href": "index.html#data",
    "title": "EDS 213: Databases and Data Management",
    "section": "Data",
    "text": "Data\nFor the database portion of the course, the Arctic Shorebird Demographics Network dataset, obtained from the Arctic Data Center, was used as a running example. While this dataset is not distributed as a relational database (it is packaged as a set of related CSV files), its structure is highly amenable to a relational approach and provides a realistic example of where and why one would want to use a relational database in the Earth and environmental sciences. It also provides just enough complexity to support realistic and complex queries and views. Note that the dataset used in the course, and included in this archive, is a cleaned-up subset of the full dataset. It is necessarily a subset of the full dataset to keep the size and complexity manageable for pedagogical purposes, and it had to be cleaned up because, unfortunately, the full dataset has many errors that would have precluded creating foreign keys.\nA class data GitHub repository, linked below, is used to distribute data files to students. Each week a new directory of files will be added to the repository and the students will be asked to pull the repository to their local environment to get the updates."
  },
  {
    "objectID": "index.html#database",
    "href": "index.html#database",
    "title": "EDS 213: Databases and Data Management",
    "section": "Database",
    "text": "Database\nDuckDB is used as the database platform due to its strict implementation of data types, which turned out to be a weakness when teaching with SQLlite last year. DuckDB is a fast in-process analytical personal database."
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "EDS 213: Databases and Data Management",
    "section": "Instructors",
    "text": "Instructors\n\nJulien Brun (jb160@ucsb.edu)\nGreg Janée (gjanee@ucsb.edu)\nRenata Curty (rcurty@ucsb.edu)"
  },
  {
    "objectID": "index.html#teaching-assistant",
    "href": "index.html#teaching-assistant",
    "title": "EDS 213: Databases and Data Management",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant\n\nJamie Miller (jkmiller@ucsb.edu)"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "EDS 213: Databases and Data Management",
    "section": "Schedule",
    "text": "Schedule\n\nClass: Monday & Wednesday 9:30-10:45 am (NCEAS)\nDiscussion - session 1: Thur 1-1:50PM, Bren Hall 1510\nDiscussion - session 2: Thur 2-2:50PM, Bren Hall 1510\nOffice hours: Monday 11-12 pm (NCEAS)"
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "EDS 213: Databases and Data Management",
    "section": "Course content",
    "text": "Course content\nSyllabus\nInstalling DuckDB\nResources\nClass data GitHub repository"
  },
  {
    "objectID": "index.html#modules",
    "href": "index.html#modules",
    "title": "EDS 213: Databases and Data Management",
    "section": "Modules",
    "text": "Modules\n\n\n\nWeek\nTopic/Content\n\n\n\n\n1\nRelational databases and data modeling\n\n\n2\nAnalyzing & cleaning the bird dataset (from csv)\n\n\n3\nIntroduction to SQL (part 1) & DuckDB\n\n\n4\nSQL part 2 + Analyzing the bird database using SQL\n\n\n5\nI/O & data management + Advanced database topics (indexing, triggers, …)\n\n\n6\nUsing R & Python to query databases + bash programming\n\n\n7\nDocumenting your work: metadata & computing environment\n\n\n8\nSensitive data\n\n\n9\nEthical & responsible data mgnt\n\n\n10\nData licensing and publication"
  }
]