[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bren MEDS 213: Databases and Data Management",
    "section": "",
    "text": "This is an archive of the materials used for a 4-unit, letter-grade course delivered in Spring 2024 as part of the Master of Environmental Data Science (MEDS) program in the Bren School of Environmental Science & Management. It includes PowerPoint presentations, instructor notes, live coding transcripts, supplemental materials and readings, and homework assignments.\nThe goals of the course were to give MEDS students the skills they need to practically, successfully, and ethically manage their data, and to create, manage, and use relational databases where appropriate. Relational database topics went farther than just SQL queries and included a significant unit on data modeling and database constraints and integrity, in addition to advanced database topics such as triggers and indexes and accessing databases from programming environments. The data management portion tied into the students’ capstone projects in a couple places, and included analyzing data from an ethical perspective, creating standards-compliant metadata, and employing data de-identification techniques. The course also included a unit on the Unix command line, with an emphasis on creating reusable Bash scripts, given in the spirit that Bash is a generally useful tool that all data scientists should have at least some familiarity with.\nFor the database portion of the course the Arctic Shorebird Demographics Network dataset, obtained from the Arctic Data Center, was used as a running example. While this dataset is not distributed as a relational database (it is packaged as a set of related CSV files), its structure is highly amenable to a relational approach and provides a realistic example of where and why one would want to use a relational database in the Earth and environmental sciences. It also provides just enough complexity to support realistic and complex queries and views. Note that the dataset used in the course, and included in this archive, is a cleaned-up subset of the full dataset. It is necessarily a subset of the full dataset to keep the size and complexity manageable for pedagogical purposes, and it had to be cleaned up because, unfortunately, the full dataset has many errors that would have precluded creating foreign keys.\nDuckDB is used as the database platform due to its strict implementation to data types that turn out to be a weakness of teaching with SQLlite last year. DuckDB is a fast in-process analytical personal database.\nA class data GitHub repository, linked below, was used as the mechanism for distributing files to students. Each week a new directory of files was added to the repository and the students were asked to pull the repository to their local environment. The repository linked here includes the files for all weeks."
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "Bren MEDS 213: Databases and Data Management",
    "section": "",
    "text": "This is an archive of the materials used for a 4-unit, letter-grade course delivered in Spring 2024 as part of the Master of Environmental Data Science (MEDS) program in the Bren School of Environmental Science & Management. It includes PowerPoint presentations, instructor notes, live coding transcripts, supplemental materials and readings, and homework assignments.\nThe goals of the course were to give MEDS students the skills they need to practically, successfully, and ethically manage their data, and to create, manage, and use relational databases where appropriate. Relational database topics went farther than just SQL queries and included a significant unit on data modeling and database constraints and integrity, in addition to advanced database topics such as triggers and indexes and accessing databases from programming environments. The data management portion tied into the students’ capstone projects in a couple places, and included analyzing data from an ethical perspective, creating standards-compliant metadata, and employing data de-identification techniques. The course also included a unit on the Unix command line, with an emphasis on creating reusable Bash scripts, given in the spirit that Bash is a generally useful tool that all data scientists should have at least some familiarity with.\nFor the database portion of the course the Arctic Shorebird Demographics Network dataset, obtained from the Arctic Data Center, was used as a running example. While this dataset is not distributed as a relational database (it is packaged as a set of related CSV files), its structure is highly amenable to a relational approach and provides a realistic example of where and why one would want to use a relational database in the Earth and environmental sciences. It also provides just enough complexity to support realistic and complex queries and views. Note that the dataset used in the course, and included in this archive, is a cleaned-up subset of the full dataset. It is necessarily a subset of the full dataset to keep the size and complexity manageable for pedagogical purposes, and it had to be cleaned up because, unfortunately, the full dataset has many errors that would have precluded creating foreign keys.\nDuckDB is used as the database platform due to its strict implementation to data types that turn out to be a weakness of teaching with SQLlite last year. DuckDB is a fast in-process analytical personal database.\nA class data GitHub repository, linked below, was used as the mechanism for distributing files to students. Each week a new directory of files was added to the repository and the students were asked to pull the repository to their local environment. The repository linked here includes the files for all weeks."
  },
  {
    "objectID": "index.html#instructors",
    "href": "index.html#instructors",
    "title": "Bren MEDS 213: Databases and Data Management",
    "section": "Instructors",
    "text": "Instructors\n\nJulien Brun (jb160@ucsb.edu)\nGreg Janée (gjanee@ucsb.edu)\nRenata Curty (rcurty@ucsb.edu)"
  },
  {
    "objectID": "index.html#teaching-assistant",
    "href": "index.html#teaching-assistant",
    "title": "Bren MEDS 213: Databases and Data Management",
    "section": "Teaching assistant",
    "text": "Teaching assistant\n\nJamie Miller (jkmiller@ucsb.edu)"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Bren MEDS 213: Databases and Data Management",
    "section": "Schedule",
    "text": "Schedule\n\nClass: Monday & Wednesday 9:30-10:45 am (NCEAS)\nDiscussion - session 1: Thur 1-1:50PM, Bren Hall 1510\nDiscussion - session 2: Thur 2-2:50PM, Bren Hall 1510\nOffice hours: Monday 11-12 pm (NCEAS)"
  },
  {
    "objectID": "index.html#course-content",
    "href": "index.html#course-content",
    "title": "Bren MEDS 213: Databases and Data Management",
    "section": "Course content",
    "text": "Course content\nSyllabus\nInstalling SQLite\nResources\nClass data GitHub repository"
  },
  {
    "objectID": "index.html#modules",
    "href": "index.html#modules",
    "title": "Bren MEDS 213: Databases and Data Management",
    "section": "Modules",
    "text": "Modules\n\n\n\nWeek\nTopic/Content\n\n\n\n\n1\nRelational databases and data modeling\n\n\n2\nAnalyzing & cleaning the bird dataset (from csv)\n\n\n3\nIntrodutcion to SQL & DuckDB\n\n\n4\nImporting data in the database\n\n\n5\nAnalyzing the bird database using SQL\n\n\n6\nUsing R & python to query databases\n\n\n7\nDocumenting your work: metdata & computing environment\n\n\n8\nSensitive data\n\n\n9\nEthical & responsible data mgnt\n\n\n10\nData licensing and publication"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Course syllabus",
    "section": "",
    "text": "This course will teach students the fundamentals of relational databases and data management. Students will learn the principles of database modeling and design and gain practical experience applying SQL (Structured Query Language) to manage and manipulate relational databases. The course also introduces the role and application of data documentation and metadata standards for interoperability and effective data management. By the end of the course, students will be equipped to make informed decisions about managing databases and data ethically and responsibly, focusing on issues such as bias, data privacy, sharing, ownership, and licensing."
  },
  {
    "objectID": "syllabus.html#overview",
    "href": "syllabus.html#overview",
    "title": "Course syllabus",
    "section": "",
    "text": "This course will teach students the fundamentals of relational databases and data management. Students will learn the principles of database modeling and design and gain practical experience applying SQL (Structured Query Language) to manage and manipulate relational databases. The course also introduces the role and application of data documentation and metadata standards for interoperability and effective data management. By the end of the course, students will be equipped to make informed decisions about managing databases and data ethically and responsibly, focusing on issues such as bias, data privacy, sharing, ownership, and licensing."
  },
  {
    "objectID": "syllabus.html#learning-objectives",
    "href": "syllabus.html#learning-objectives",
    "title": "Course syllabus",
    "section": "Learning objectives",
    "text": "Learning objectives\n\nUnderstand the fundamental principles of relational databases and relational data modeling, including table structures, primary and foreign keys, relationships between tables, and data normalization.\nUnderstand how to use the Unix command line and how manage DuckDB databases from the command line.\nUse SQL to retrieve, manipulate, and manage data stored in a relational database.\nDemonstrate proficiency in querying, filtering, sorting, and programmatically accessing and interacting with relational databases from R and Python.\nBecome familiar with advanced database topics such as concurrency, transactions, indexing, backups, and publication.\nUnderstand the role of good data documentation and metadata standards for interoperability, effective data management, and reproducibility.\nOperationalize the FAIR principles into data management practices.\nProduce a metadata record in EML (Ecological Metadata Language) and apply metadata crosswalks to programmatically convert metadata schemas.\nUnderstand the ethics of sensitive data and how to de-identify sensitive data.\nEvaluate ethical and responsible data management practices, including bias, data privacy, sharing, ownership, and licensing issues."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "Course syllabus",
    "section": "Schedule",
    "text": "Schedule\n\nClass: Monday & Wednesday 9:30-10:45 am (NCEAS)\nDiscussion - session 1: Thur 1-1:50PM, Bren Hall 1510\nDiscussion - session 2: Thur 2-2:50PM, Bren Hall 1510\nOffice hours: Monday 11-12 pm (NCEAS)"
  },
  {
    "objectID": "syllabus.html#modules",
    "href": "syllabus.html#modules",
    "title": "Course syllabus",
    "section": "Modules",
    "text": "Modules\n\n\n\nWeek\nTopic/Content\n\n\n\n\n1\nRelational databases and data modeling\n\n\n2\nAnalyzing & cleaning the bird dataset (from csv)\n\n\n3\nIntrodutcion to SQL & DuckDB\n\n\n4\nImporting data in the database\n\n\n5\nAnalyzing the bird database using SQL\n\n\n6\nUsing R & python to query databases\n\n\n7\nDocumenting your work: metdata & computing environment\n\n\n8\nSensitive data\n\n\n9\nEthical & responsible data mgnt\n\n\n10\nData licensing and publication\n\n\n\n*Schedule subject to change."
  },
  {
    "objectID": "syllabus.html#course-assessment",
    "href": "syllabus.html#course-assessment",
    "title": "Course syllabus",
    "section": "Course assessment",
    "text": "Course assessment\nYour performance in this course will depend 90% on weekly homework assignments and quizzes and 10% on participation. There will be no graded exercises or homework the last week."
  },
  {
    "objectID": "syllabus.html#attendance-and-homework-policy",
    "href": "syllabus.html#attendance-and-homework-policy",
    "title": "Course syllabus",
    "section": "Attendance and homework policy",
    "text": "Attendance and homework policy\nAttendance is required. Material will be given in class that is not covered by slides or background readings.\nHomework is expected to be turned in on time. A generous amount of time will be given to complete assignments. Do not wait until the last minute to work on homework in case something unexpected comes up. Homework turned in late will be docked 25% per day."
  },
  {
    "objectID": "syllabus.html#code-of-conduct",
    "href": "syllabus.html#code-of-conduct",
    "title": "Course syllabus",
    "section": "Code of conduct",
    "text": "Code of conduct\nAll students are expected to read and comply with the UCSB Student Conduct Code. We expect cooperation from all members to help ensure a welcoming and inclusive environment for everybody. We are determined to make our courses welcoming, inclusive and harassment-free for everyone regardless of gender, gender identity and expression, race, age, sexual orientation, disability, physical appearance, or religion (or lack thereof). We do not tolerate harassment of class participants, teaching assistants, or instructors in any form. Derogatory, abusive, or demeaning language or imagery will not be tolerated."
  },
  {
    "objectID": "syllabus.html#student-support",
    "href": "syllabus.html#student-support",
    "title": "Course syllabus",
    "section": "Student support",
    "text": "Student support\nWe understand that ongoing crises impact students differently based on experiences, identities, living situations and resources, family responsibilities, and unforeseen challenges. We encourage you to prioritize your well-being. We are here to help you reach your learning and career goals. You are always welcome to reach out to our teaching team so that we can best support you. Please see the UCSB Campus Resource Guide for campus student support and services."
  },
  {
    "objectID": "syllabus.html#disabled-students-program",
    "href": "syllabus.html#disabled-students-program",
    "title": "Course syllabus",
    "section": "Disabled students program",
    "text": "Disabled students program\nStudents with disabilities and/or alternative learning needs are encouraged to work with the Disabled Students Program at UCSB to ensure we can best support your learning and success."
  },
  {
    "objectID": "modules/week01/index-01.html",
    "href": "modules/week01/index-01.html",
    "title": "Week 1 - Relational databases and data modeling",
    "section": "",
    "text": "Benefits of relational databases\nRelational data model and SQL data definition\nData modeling"
  },
  {
    "objectID": "modules/week01/index-01.html#learning-objectives",
    "href": "modules/week01/index-01.html#learning-objectives",
    "title": "Week 1 - Relational databases and data modeling",
    "section": "",
    "text": "Benefits of relational databases\nRelational data model and SQL data definition\nData modeling"
  },
  {
    "objectID": "modules/week01/index-01.html#slides",
    "href": "modules/week01/index-01.html#slides",
    "title": "Week 1 - Relational databases and data modeling",
    "section": "Slides",
    "text": "Slides\nslides-01.pptx"
  },
  {
    "objectID": "modules/week01/index-01.html#resources",
    "href": "modules/week01/index-01.html#resources",
    "title": "Week 1 - Relational databases and data modeling",
    "section": "Resources",
    "text": "Resources\n\nhttps://learning.nceas.ucsb.edu/2022-04-arctic/data-modeling-essentials.html\n\nVery brief introduction to data modeling, ties into “tidy data.”\n\nChristoph Wohner, Johannes Peterseil, and Hermann Klug (2022). Designing and implementing a data model for describing environmental monitoring and research sites. Ecological Informatics 70, 101708.\nhttps://doi.org/10.1016/j.ecoinf.2022.101708\n\nGood case study.\n\nGerald A. Burnette (2022). Managing environmental data: principles, techniques, and best practices. CRC Press.\nAccess via Library Catalog\n\nComprehensive text, specific to environmental sciences.\n\nGraeme C. Simsion and Graham C. Witt (2005). Data Modeling Essentials. 3rd ed. Amsterdam: Morgan Kaufmann.\nAccess via Library Catalog\nGoogle Books\n\nComprehensive text, not specific to the environmental sciences.\n\nHartmut Hebbel (1994). Environmental data modeling. Annals of Operations Research 54, 263-278.\nhttps://doi.org/10.1007/BF02031737\n\nA broader view of data organization.\n\nJeffrey D. Ullman and Jennifer Widom (2008). A First Course in Database Systems. 3rd ed. Upper Saddle River, NJ: Pearson/Prentice Hall.\nAccess via Library Catalog\n\nComplete but theoretical introduction to relational databases, data modeling, and relational algebra."
  },
  {
    "objectID": "modules/week01/index-01.html#homework",
    "href": "modules/week01/index-01.html#homework",
    "title": "Week 1 - Relational databases and data modeling",
    "section": "Homework",
    "text": "Homework\nData modeling exercise"
  },
  {
    "objectID": "modules/week03/hw-03-1.html",
    "href": "modules/week03/hw-03-1.html",
    "title": "Week 3 - SQL problem 1",
    "section": "",
    "text": "It’s a useful skill in life (I’m not being rhetorical, I really mean that, it’s a useful skill) to be able to construct an experiment to answer a hypothesis. Suppose you’re not sure what the AVG function returns if there are NULL values in the column being averaged. Suppose you either didn’t have access to any documentation, or didn’t trust it. What experiment could you run to find out what happens?\nThere are two parts to this problem.\n\nPart 1\nConstruct an SQL experiment to determine the answer to the question above. Does SQL abort with some kind of error? Does it ignore NULL values? Do the NULL values somehow factor into the calculation, and if so, how?\nI would suggest you start by creating a table (in the bird database, in a new database, in a transient in-memory database, doesn’t matter) with a single column that has data type REAL (for part 2 below, it must be REAL). You can make your table a temp table or not, your choice.\nCREATE TEMP TABLE mytable... ;\nNow insert some real numbers and at least one NULL into your table.\nINSERT INTO mytable... ;\n(Hmm, can you insert multiple rows at once, or do you have to do a separate INSERT for each row?)\nOnce you have your little table constructed, try doing an AVG on the column and see what is returned. What would the average be if the function ignored NULLs? What would the average be if it somehow factored them in? What is actually returned?\nPlease submit both your SQL and your answer to the question about how AVG operates in the presence of NULL values.\n\n\nPart 2\nIf SQL didn’t have an AVG function, you could compute the average value of a column by doing something like this on your table:\nSELECT SUM(mycolumn)/COUNT(*) FROM mytable;\nSELECT SUM(mycolumn)/COUNT(mycolumn) FROM mytable;\nWhich query above is correct? Please explain why.\nNow that you’re done with your table, you can delete it if desired:\nDROP TABLE mytable;"
  },
  {
    "objectID": "modules/week03/hw-03-2.html",
    "href": "modules/week03/hw-03-2.html",
    "title": "Week 3 - SQL problem 2",
    "section": "",
    "text": "If you want to know which site has the largest area, it’s tempting to say\nSELECT Site_name, MAX(Area) FROM Site;\nbut as explained in class, databases will correctly compute the maximum but will select an arbitrary row to fill in the Site_name column. No good! This misleading behavior is more apparent if we do an average instead of a maximum:\nSELECT Site_name, AVG(Area) FROM Site;\n\n┌───────────┬───────────┐\n│ Site_name │ AVG(Area) │\n├───────────┼───────────┤\n│ Barrow    │ 440.6125  │\n└───────────┴───────────┘\nfor there is no site whose area exactly equals the average, and so there is nothing you could reasonably put in Site_name, and it certainly wouldn’t be Barrow. (SQLite is special in that if you do a MIN or MAX, it will return the row (or one of the rows, if there are multiple rows) that matches the minimum or maximum. But other databases do not do that.) So, we need a plan B.\n\nPart 1\nFind the site name and area of the site having the largest area. Do so by ordering the rows in a particularly convenient order, and using LIMIT to select just the first row. Your result should look like:\n┌──────────────┬────────┐\n│  Site_name   │  Area  │\n├──────────────┼────────┤\n│ Coats Island │ 1239.1 │\n└──────────────┴────────┘\nPlease submit your SQL.\n\n\nPart 2\nDo the same, but use a nested query. First, create a query that finds the maximum area. Then, create a query that selects the site name and area of the site whose area equals the maximum. Your overall query will look something like:\nSELECT Site_name, Area FROM Site WHERE Area = (SELECT ...);"
  },
  {
    "objectID": "installing-duckdb.html",
    "href": "installing-duckdb.html",
    "title": "Installing duckDB",
    "section": "",
    "text": "DuckDB has been installed on the MEDS server. We also recommend to install it on your personal machine following those instructions: https://duckdb.org/docs/installation/"
  },
  {
    "objectID": "installing-duckdb.html#installation",
    "href": "installing-duckdb.html#installation",
    "title": "Installing duckDB",
    "section": "",
    "text": "DuckDB has been installed on the MEDS server. We also recommend to install it on your personal machine following those instructions: https://duckdb.org/docs/installation/"
  },
  {
    "objectID": "installing-duckdb.html#visual-code-integration-optional",
    "href": "installing-duckdb.html#visual-code-integration-optional",
    "title": "Installing duckDB",
    "section": "Visual code integration (optional)",
    "text": "Visual code integration (optional)\nYou can use DuckDB directly from the terminal but if you also want to have the option to have both a SQL script and the terminal open, we recommend to use visual code. There is one setting to be done to link the script and the terminal:\nIn Visual Code:\n\nopen the palette Shift+Cmd+P and search for: Preferences: Open Keyboard Shortcuts (JSON) \nenter the following text & save\n\n// Place your key bindings in this file to override the defaults\n[\n    {\n        \"key\": \"shift+enter\",\n        \"command\": \"workbench.action.terminal.runSelectedText\"\n    }\n]\nNow you can hit Shift+Return at then end of a line in your SQL script and it should run the command directly in the terminal!"
  },
  {
    "objectID": "installing-duckdb.html#test",
    "href": "installing-duckdb.html#test",
    "title": "Installing duckDB",
    "section": "Test",
    "text": "Test\nLet’s test our new installation using visual code\n\nopen a terminal from the terminal menu -&gt; new terminal\nIn the terminal type duckdb, this should start duckdb\nOpen a New text file: file menu -&gt; new text file\nCopy paste the following code:\n\n-- Start the DB at the terminal: duckdb\n\nCREATE TABLE ducks AS SELECT 3 As age, 'mandarin' AS breed;\n\nSHOW TABLES;\n\nFROM ducks SELECT *;\n\nUse Shift+Return to run the SQL code line by line\n\nYou should have something that looks like this:"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Burnette. (2022). Managing environmental data: principles, techniques, and best practices. CRC Press. https://search.library.ucsb.edu/permalink/01UCSB_INST/1aqck9j/alma9917095897506531\nChapman, AD, & Grafton, O. (2008). Guide to Best Practices for Generalising Sensitive Species-Occurrence Data (v1.0). https://doi.org/10.15468/doc-b02j-gt10\nCrystal-Ornelas, R., Varadharajan, C., O’Ryan, D., Ramírez-Muñoz, J., Jones, M. B., Lehnert, K. A., … & Servilla, M. (2022). Enabling FAIR data in Earth and environmental science with community-centric (meta)data reporting formats. Scientific Data, 9(1), 700. https://doi.org/10.1038/s41597-022-01606-w\nJones, M. B., O’Brien, M., Mecum, B., Boettiger, C., Schildhauer, M., Maier, M., Whiteaker, T., Earl, S., & Chong, S. (2019). Ecological Metadata Language version 2.2.0. KNB Data Repository. https://doi.org/10.5063/F11834T2\nLabastida, I., & Margoni, T. (2020). Licensing FAIR Data for Reuse. Data Intelligence, 2(1-2), 199-207. https://doi.org/10.1162/dint_a_00042\nMcGovern, A., Ebert-Uphoff, I., Gagne, D., & Bostrom, A. (2022). Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science. Environmental Data Science, 1, E6. https://doi.org/10.1017/eds.2022.5\nRecknagel, F., & Michener, W. K. (Eds.). (2018). Ecological informatics: Data management and knowledge discovery (3rd ed.). Springer."
  },
  {
    "objectID": "resources.html#bibliography",
    "href": "resources.html#bibliography",
    "title": "Resources",
    "section": "",
    "text": "Burnette. (2022). Managing environmental data: principles, techniques, and best practices. CRC Press. https://search.library.ucsb.edu/permalink/01UCSB_INST/1aqck9j/alma9917095897506531\nChapman, AD, & Grafton, O. (2008). Guide to Best Practices for Generalising Sensitive Species-Occurrence Data (v1.0). https://doi.org/10.15468/doc-b02j-gt10\nCrystal-Ornelas, R., Varadharajan, C., O’Ryan, D., Ramírez-Muñoz, J., Jones, M. B., Lehnert, K. A., … & Servilla, M. (2022). Enabling FAIR data in Earth and environmental science with community-centric (meta)data reporting formats. Scientific Data, 9(1), 700. https://doi.org/10.1038/s41597-022-01606-w\nJones, M. B., O’Brien, M., Mecum, B., Boettiger, C., Schildhauer, M., Maier, M., Whiteaker, T., Earl, S., & Chong, S. (2019). Ecological Metadata Language version 2.2.0. KNB Data Repository. https://doi.org/10.5063/F11834T2\nLabastida, I., & Margoni, T. (2020). Licensing FAIR Data for Reuse. Data Intelligence, 2(1-2), 199-207. https://doi.org/10.1162/dint_a_00042\nMcGovern, A., Ebert-Uphoff, I., Gagne, D., & Bostrom, A. (2022). Why we need to focus on developing ethical, responsible, and trustworthy artificial intelligence approaches for environmental science. Environmental Data Science, 1, E6. https://doi.org/10.1017/eds.2022.5\nRecknagel, F., & Michener, W. K. (Eds.). (2018). Ecological informatics: Data management and knowledge discovery (3rd ed.). Springer."
  },
  {
    "objectID": "modules/week03/index-03.html",
    "href": "modules/week03/index-03.html",
    "title": "Week 3 - SQLite and SQL",
    "section": "",
    "text": "Understand the relationship of SQL to relational databases\nUnderstand how SQLite differs from client/server databases\nUnderstand basic SQL syntax and statements\nBe able to answer basic questions about data"
  },
  {
    "objectID": "modules/week03/index-03.html#learning-objectives",
    "href": "modules/week03/index-03.html#learning-objectives",
    "title": "Week 3 - SQLite and SQL",
    "section": "",
    "text": "Understand the relationship of SQL to relational databases\nUnderstand how SQLite differs from client/server databases\nUnderstand basic SQL syntax and statements\nBe able to answer basic questions about data"
  },
  {
    "objectID": "modules/week03/index-03.html#slides-and-other-materials",
    "href": "modules/week03/index-03.html#slides-and-other-materials",
    "title": "Week 3 - SQLite and SQL",
    "section": "Slides and other materials",
    "text": "Slides and other materials\nslides-03.pptx\nLecture notes:\n\nlecture-notes-03-mon.txt\nlecture-notes-03-wed.txt\n\nASDN dataset ER (entity-relationship) diagram\nClass data GitHub repository, week 3"
  },
  {
    "objectID": "modules/week03/index-03.html#resources",
    "href": "modules/week03/index-03.html#resources",
    "title": "Week 3 - SQLite and SQL",
    "section": "Resources",
    "text": "Resources\n\nhttp://swcarpentry.github.io/sql-novice-survey/\n\nGood Carpentry lesson, our lesson is drawn from this.\n\nC.J. Date and Hugh Darwen (1993). A Guide to the SQL Standard. 3rd ed. Reading, MA: Addison-Wesley.\nAccess via Library Catalog\n\nThe ANSI standard.\n\nJoe Celko (1995). Joe Celko’s SQL For Smarties: Advanced SQL Programming. San Francisco, CA: Morgan Kaufmann.\nAccess via Library Catalog\n\nThis guy is an SQL guru. Newer versions of this book are available online, check the Library catalog (a bug is preventing me from linking directly).\n\nGrant Allen and Mike Owens (2010). The Definitive Guide to SQLite. 2nd ed. Berkeley, CA: Apress.\nAccess via Library Catalog\n\nGood reference. Can access online!\n\nJeffrey D. Ullman and Jennifer Widom (2008). A First Course in Database Systems. 3rd ed. Upper Saddle River, NJ: Pearson/Prentice Hall.\nAccess via Library Catalog\n\nComplete but theoretical introduction to relational databases, data modeling, and relational algebra."
  },
  {
    "objectID": "modules/week03/index-03.html#homework",
    "href": "modules/week03/index-03.html#homework",
    "title": "Week 3 - SQLite and SQL",
    "section": "Homework",
    "text": "Homework\nSQL problem 1\nSQL problem 2\nSQL problem 3"
  },
  {
    "objectID": "modules/week03/hw-03-3.html",
    "href": "modules/week03/hw-03-3.html",
    "title": "Week 3 - SQL problem 3",
    "section": "",
    "text": "Your mission, should you choose to accept it, is to list the scientific names of bird species in descending order of their maximum average egg volumes. That is, compute the average volume of the eggs in each nest, and then for the nests of each species compute the maximum of those average volumes, and list by species in descending order of maximum volume. You final table should look like:\n┌─────────────────────────┬──────────────────┐\n│     Scientific_name     │  Max_avg_volume  │\n├─────────────────────────┼──────────────────┤\n│ Pluvialis squatarola    │ 36541.8531755592 │\n│ Pluvialis dominica      │ 33847.8550096089 │\n│ Arenaria interpres      │ 23338.620942275  │\n│ Calidris fuscicollis    │ 13277.1428014342 │\n│ Calidris alpina         │ 12196.2368406617 │\n│ Charadrius semipalmatus │ 11266.9746753467 │\n│ Phalaropus fulicarius   │ 8906.7745740725  │\n└─────────────────────────┴──────────────────┘\n(By the way, regarding the leader in egg size above, Birds of the World says that Pluvialis squatarola’s eggs are “Exceptionally large for size of female (ca. 16% weight of female)”.)\nTo calculate the volume of an egg, use the simplified formula\n\\[{\\pi \\over 6} W^2 L\\]\nwhere \\(W\\) is the egg width and \\(L\\) is the egg length. You can use 3.14 for \\(\\pi\\). (The real formula takes into account the ovoid shape of eggs, but only width and length are available to us here.)\nA good place to start is just to group bird eggs by nest (i.e., Nest_ID) and compute average volumes:\nCREATE TEMP TABLE Averages AS\n    SELECT Nest_ID, AVG(...) AS Avg_volume\n        FROM ...\n        GROUP BY ...;\nYou can now join that table with Bird_nests, so that you can group by species, and also join with the Species table to pick up scientific names. To do just the first of those joins, you could say something like\nSELECT Species, MAX(...)\n    FROM Bird_nests, Averages USING (Nest_ID)\n    GROUP BY ...;\nThat’s not the whole story, we want scientific names not species codes. Another join is needed. A couple strategies here. One, you can modify the above query to also join with the Species table (you’ll need to replace USING with ON …). Two, you can save the above as another temp table and join it to Species separately.\nDon’t forget to order the results. Here it is convenient to give computed quantities nice names so you can refer to them.\nPlease submit all of the SQL you used to solve the problem. Bonus points if you can do all of the above in one statement."
  },
  {
    "objectID": "modules/week01/hw-01.html",
    "href": "modules/week01/hw-01.html",
    "title": "Week 1 - Data modeling exercise",
    "section": "",
    "text": "Create a table definition for the Snow_survey table that is maximally expressive, that is, that captures as much of the semantics and characteristics of the data using SQL’s data definition language as is possible.\nIn the class data GitHub repository, week 1 directory you will find the table described in the metadata (consult 01_ASDN_Readme.txt) and the data can be found in ASDN_Snow_survey.csv. You will want to look at the values that occur in the data using a tool like R, Python, or OpenRefine.\nPlease consider:\nYou may (or may not) want to take advantage of the Species, Site, Color_band_code, and Personnel supporting tables. These are also documented in the metadata, and SQL table definitions for them have already been created and are included below.\nPlease express your table definition in SQL, but don’t worry about getting the SQL syntax exactly correct. This assignment is just a thought exercise. If you do want to try to write correct SQL, though, your may find it helpful to consult the DuckDB CREATE TABLE documentation.\nFinally, please provide some explanation for why you made the choices you did, and any questions or uncertainties you have. Don’t write an essay! Bullet points are sufficient."
  },
  {
    "objectID": "modules/week01/hw-01.html#appendix",
    "href": "modules/week01/hw-01.html#appendix",
    "title": "Week 1 - Data modeling exercise",
    "section": "Appendix",
    "text": "Appendix\nCREATE TABLE Species (\n    Code TEXT PRIMARY KEY,\n    Common_name TEXT UNIQUE NOT NULL,\n    Scientific_name TEXT,\n    Relevance TEXT\n);\n\nCREATE TABLE Site (\n    Code TEXT PRIMARY KEY,\n    Site_name TEXT UNIQUE NOT NULL,\n    Location TEXT NOT NULL,\n    Latitude REAL NOT NULL CHECK (Latitude BETWEEN -90 AND 90),\n    Longitude REAL NOT NULL CHECK (Longitude BETWEEN -180 AND 180),\n    \"Total_Study_Plot_Area_(ha)\" REAL NOT NULL\n        CHECK (\"Total_Study_Plot_Area_(ha)\" &gt; 0),\n    UNIQUE (Latitude, Longitude)\n);\n\nCREATE TABLE Color_band_code (\n    Code TEXT PRIMARY KEY,\n    Color TEXT NOT NULL UNIQUE\n);\n\nCREATE TABLE Personnel (\n    Abbreviation TEXT PRIMARY KEY,\n    Name TEXT NOT NULL UNIQUE\n);"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About RDS",
    "section": "",
    "text": "Research Data Services (RDS) helps UCSB researchers manage and preserve their research data through:\n\nConsultations\nLong-term engagements\nInstructional workshops\n\nOur team offers support across the research data lifecycle, from pre-project planning to post-project archival, connecting researchers with both locally- and externally-provided curation services. Our goal is to ensure that all research data is well-described, FAIR (Findable, Accessible, Interoperable, Reusable), and sustainably preservable, and that researchers receive scholarly credit for sharing and publishing data.\nContact us if you have any questions: rds@library.ucsb.edu"
  }
]